{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfeU6OYcnH68",
        "outputId": "e29fe279-7be0-41ae-a95e-d3c4279b49ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kyle-anthonyhay/Documents/CODE/Revature-Training/Ai-Engineering/December/venv/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(np, \"object\"):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Libraries imported!\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "âœ… Data loaded!\n",
            "   Training samples: 20000\n",
            "   Test samples: 5000\n",
            "   Sequence length: 100 words\n",
            "\n",
            "ğŸ“š Sample reviews:\n",
            "\n",
            "ğŸ“ Review #0 (POSITIVE ğŸ˜Š)\n",
            "--------------------------------------------------\n",
            "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNKNOWN> is an amazing actor and now the same being director <UNKNOWN> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNKNOWN> ...\n",
            "\n",
            "ğŸ“ Review #1 (NEGATIVE ğŸ˜)\n",
            "--------------------------------------------------\n",
            "<START> big hair big <UNKNOWN> bad music and a giant safety <UNKNOWN> these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an <UNKNOWN> the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are <UNKNOWN> and funny in equal <UNKNOWN> t...\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# MODEL PERSISTENCE & TRAINING OPTIMIZATION\n",
        "# =============================================================================\n",
        "# Colab notebook for teaching saving, checkpoints, early stopping, overfitting\n",
        "# Uses IMDB sentiment analysis throughout\n",
        "# =============================================================================\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 1: SETUP & LOAD DATA\n",
        "# =============================================================================\n",
        "# Run this first - sets up everything for all demos\n",
        "\n",
        "!pip install -q tensorflow\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "print(\"âœ… Libraries imported!\")\n",
        "\n",
        "# Load IMDB dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=5000)\n",
        "\n",
        "# Use subset for faster training\n",
        "x_train, y_train = x_train[:20000], y_train[:20000]\n",
        "x_test, y_test = x_test[:5000], y_test[:5000]\n",
        "\n",
        "# Save original for showing reviews\n",
        "x_train_original = x_train.copy()\n",
        "x_test_original = x_test.copy()\n",
        "\n",
        "# Pad sequences to same length\n",
        "x_train = keras.utils.pad_sequences(x_train, maxlen=100)\n",
        "x_test = keras.utils.pad_sequences(x_test, maxlen=100)\n",
        "\n",
        "# Word index for converting text\n",
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "index_to_word = {idx + 3: word for word, idx in word_index.items()}\n",
        "index_to_word[0] = '<PAD>'\n",
        "index_to_word[1] = '<START>'\n",
        "index_to_word[2] = '<UNKNOWN>'\n",
        "\n",
        "def show_review(index, dataset='train'):\n",
        "    \"\"\"Show a review in readable text.\"\"\"\n",
        "    if dataset == 'train':\n",
        "        data, labels = x_train_original, y_train\n",
        "    else:\n",
        "        data, labels = x_test_original, y_test\n",
        "    text = ' '.join([index_to_word.get(num, '?') for num in data[index]])\n",
        "    sentiment = \"POSITIVE ğŸ˜Š\" if labels[index] == 1 else \"NEGATIVE ğŸ˜\"\n",
        "    print(f\"\\nğŸ“ Review #{index} ({sentiment})\")\n",
        "    print(\"-\" * 50)\n",
        "    print(text[:500] + \"...\" if len(text) > 500 else text)\n",
        "\n",
        "print(f\"\\nâœ… Data loaded!\")\n",
        "print(f\"   Training samples: {len(x_train)}\")\n",
        "print(f\"   Test samples: {len(x_test)}\")\n",
        "print(f\"   Sequence length: {x_train.shape[1]} words\")\n",
        "\n",
        "print(\"\\nğŸ“š Sample reviews:\")\n",
        "show_review(0)\n",
        "show_review(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UC0IXJOrywh",
        "outputId": "5779090b-f2da-47db-e166-c37e2505941b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=============================================================================\n",
            "THREE WAYS TO SAVE A MODEL\n",
            "=============================================================================\n",
            "\n",
            "1. SavedModel (folder)\n",
            "   â””â”€â”€ my_model/\n",
            "       â”œâ”€â”€ saved_model.pb      â† Architecture\n",
            "       â”œâ”€â”€ variables/          â† Weights\n",
            "       â””â”€â”€ assets/             â† Optimizer state\n",
            "\n",
            "   Use for: Production, TensorFlow Serving\n",
            "\n",
            "2. H5 (single file)\n",
            "   â””â”€â”€ my_model.h5             â† Everything in one file\n",
            "\n",
            "   Use for: Sharing, simple projects\n",
            "\n",
            "3. Weights only (smallest)\n",
            "   â””â”€â”€ my_weights.h5           â† Just the numbers\n",
            "\n",
            "   Use for: Transfer learning\n",
            "   âš ï¸  Must rebuild architecture before loading!\n",
            "\n",
            "=============================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 2: THREE FORMATS EXPLAINED\n",
        "# =============================================================================\n",
        "# Visual explanation of what each format contains\n",
        "\n",
        "print(\"\"\"\n",
        "=============================================================================\n",
        "THREE WAYS TO SAVE A MODEL\n",
        "=============================================================================\n",
        "\n",
        "1. SavedModel (folder)\n",
        "   â””â”€â”€ my_model/\n",
        "       â”œâ”€â”€ saved_model.pb      â† Architecture\n",
        "       â”œâ”€â”€ variables/          â† Weights\n",
        "       â””â”€â”€ assets/             â† Optimizer state\n",
        "\n",
        "   Use for: Production, TensorFlow Serving\n",
        "\n",
        "2. H5 (single file)\n",
        "   â””â”€â”€ my_model.h5             â† Everything in one file\n",
        "\n",
        "   Use for: Sharing, simple projects\n",
        "\n",
        "3. Weights only (smallest)\n",
        "   â””â”€â”€ my_weights.h5           â† Just the numbers\n",
        "\n",
        "   Use for: Transfer learning\n",
        "   âš ï¸  Must rebuild architecture before loading!\n",
        "\n",
        "=============================================================================\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "7xi7Vx9Rr7nW",
        "outputId": "1e2180ba-fe9f-4936-9784-5e45e5f7348d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TRAINING SENTIMENT MODEL\n",
            "============================================================\n",
            "\n",
            "ğŸ“Š Model Architecture:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kyle-anthonyhay/Documents/CODE/Revature-Training/Ai-Engineering/December/venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:100: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sentiment_model\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sentiment_model\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ output (\u001b[38;5;33mDense\u001b[0m)                  â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ‹ï¸ Training...\n",
            "Epoch 1/3\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.6976 - loss: 0.6073 - val_accuracy: 0.7965 - val_loss: 0.4688\n",
            "Epoch 2/3\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8180 - loss: 0.4221 - val_accuracy: 0.8155 - val_loss: 0.4021\n",
            "Epoch 3/3\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8564 - loss: 0.3490 - val_accuracy: 0.8273 - val_loss: 0.3855\n",
            "\n",
            "âœ… Training complete! Test accuracy: 82.00%\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 3: TRAIN MODEL\n",
        "# =============================================================================\n",
        "# Train a sentiment model we'll use for saving demos\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING SENTIMENT MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Build model\n",
        "model = keras.Sequential([\n",
        "    layers.Embedding(5000, 32, input_length=100, name='embedding'),\n",
        "    layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2, name='lstm'),\n",
        "    layers.Dense(1, activation='sigmoid', name='output')\n",
        "], name='sentiment_model')\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\nğŸ“Š Model Architecture:\")\n",
        "model.summary()\n",
        "\n",
        "print(\"\\nğŸ‹ï¸ Training...\")\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=3,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"\\nâœ… Training complete! Test accuracy: {test_acc:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qSXO9xyPsBcK",
        "outputId": "a24c06b6-37c3-4884-8733-011b4e1f864b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SAVING MODEL - THREE WAYS\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 4: SAVE MODEL - 3 WAYS\n",
        "# =============================================================================\n",
        "# Demonstrate all three saving methods\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"SAVING MODEL - THREE WAYS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create directory\n",
        "os.makedirs('saved_models-1', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0XYZUKjjsXHc",
        "outputId": "bc59a90f-dd18-4c9b-b58d-59a9a75ea3db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Keras format saved to: saved_models/sentiment_model.keras\n"
          ]
        }
      ],
      "source": [
        "# Method 1: Keras format (recommended - new default)\n",
        "model.save('saved_models-1/sentiment_model.keras')\n",
        "print(\"\\nâœ… Keras format saved to: saved_models/sentiment_model.keras\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BT2iJJTQtJh9",
        "outputId": "1b1b7071-71b5-4f97-9777-0d7b63520a14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… H5 format saved to: saved_models/sentiment_model.h5\n"
          ]
        }
      ],
      "source": [
        "# Method 2: H5 format (legacy but still works)\n",
        "model.save('saved_models/sentiment_model.h5')\n",
        "print(\"âœ… H5 format saved to: saved_models/sentiment_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7Ew2-zGTtMAi",
        "outputId": "b73cefe2-0104-4df6-e40b-00ff8f3c8687"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Weights saved to: saved_models/sentiment_weights.weights.h5\n"
          ]
        }
      ],
      "source": [
        "# Method 3: Weights only\n",
        "model.save_weights('saved_models/sentiment_weights.weights.h5')\n",
        "print(\"âœ… Weights saved to: saved_models/sentiment_weights.weights.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SAtpLpM9tS1w",
        "outputId": "cecb928d-70e6-4254-ae10-e954bb343598"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_models/sentiment_export/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_models/sentiment_export/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at 'saved_models/sentiment_export'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 100), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  5171873168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5039548048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5171872784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5171875088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5171872976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5171874896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5171875664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "âœ… Exported for TFServing to: saved_models/sentiment_export/\n"
          ]
        }
      ],
      "source": [
        "# Method 4: Export for TFServing/TFLite (production deployment)\n",
        "model.export('saved_models/sentiment_export')\n",
        "print(\"âœ… Exported for TFServing to: saved_models/sentiment_export/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-rpx63-MtU8U",
        "outputId": "80639a16-c3e1-4202-c1a9-5d2c7fa4a440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“ File sizes:\n",
            "total 8016\n",
            "drwxr-xr-x@ 6 kyle-anthonyhay  staff   192B Dec 30 11:31 \u001b[34msentiment_export\u001b[m\u001b[m\n",
            "-rw-r--r--@ 1 kyle-anthonyhay  staff   2.0M Dec 30 11:31 sentiment_model.h5\n",
            "-rw-r--r--@ 1 kyle-anthonyhay  staff   1.9M Dec 30 11:31 sentiment_weights.weights.h5\n"
          ]
        }
      ],
      "source": [
        "# Show file sizes\n",
        "print(\"\\nğŸ“ File sizes:\")\n",
        "!ls -lh saved_models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBfMeFhgtXV_",
        "outputId": "258b9ab7-983c-42f4-f11b-06c88942d18f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "LOADING MODEL & MAKING PREDICTIONS\n",
            "============================================================\n",
            "âœ… Model loaded from H5 file!\n",
            "\n",
            "ğŸ”® Predictions on new reviews:\n",
            "\n",
            "ğŸ“ \"This movie was amazing! I loved every minute of it...\"\n",
            "   â†’ 58.9% confident â†’ POSITIVE ğŸ˜Š\n",
            "\n",
            "ğŸ“ \"Terrible film. Waste of time and money. Do not wat...\"\n",
            "   â†’ 5.2% confident â†’ NEGATIVE ğŸ˜\n",
            "\n",
            "ğŸ“ \"It was okay. Nothing special but not bad either.\"\n",
            "   â†’ 8.0% confident â†’ NEGATIVE ğŸ˜\n",
            "\n",
            "ğŸ“ \"Best movie I have ever seen! Absolutely brilliant!\"\n",
            "   â†’ 53.1% confident â†’ POSITIVE ğŸ˜Š\n",
            "\n",
            "ğŸ“ \"Boring and slow. I fell asleep halfway through.\"\n",
            "   â†’ 9.6% confident â†’ NEGATIVE ğŸ˜\n",
            "\n",
            "============================================================\n",
            "ğŸ’¡ TRY YOUR OWN:\n",
            "============================================================\n",
            "ğŸ“ \"I really enjoyed this film, great acting and story\"\n",
            "   â†’ 83.1% confident â†’ POSITIVE ğŸ˜Š\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 5: LOAD AND PREDICT\n",
        "# =============================================================================\n",
        "# Load saved model and predict on NEW sentences\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"LOADING MODEL & MAKING PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load model\n",
        "loaded_model = keras.models.load_model('saved_models-1/sentiment_model.keras')\n",
        "print(\"âœ… Model loaded from H5 file!\")\n",
        "\n",
        "# Function to predict on any text\n",
        "def predict_sentiment(text):\n",
        "    \"\"\"Predict sentiment of a new review.\"\"\"\n",
        "    # Convert text to lowercase and split into words\n",
        "    words = text.lower().split()\n",
        "\n",
        "    # \"I want you to remove this for loop and use tokenizer\" .\n",
        "    # Convert words to numbers using word_index\n",
        "    sequence = []\n",
        "    for word in words:\n",
        "        idx = word_index.get(word)\n",
        "        if idx is not None and idx < 5000:  # Only use top 5000 words\n",
        "            sequence.append(idx + 3)  # Offset by 3 (reserved indices)\n",
        "        else:\n",
        "            sequence.append(2)  # Unknown word\n",
        "\n",
        "    # Pad to length 100\n",
        "    padded = keras.utils.pad_sequences([sequence], maxlen=100)\n",
        "\n",
        "    # Predict\n",
        "    prob = loaded_model.predict(padded, verbose=0)[0][0]\n",
        "    sentiment = \"POSITIVE ğŸ˜Š\" if prob > 0.5 else \"NEGATIVE ğŸ˜\"\n",
        "\n",
        "    return prob, sentiment\n",
        "\n",
        "# Test reviews - try your own!\n",
        "test_reviews = [\n",
        "    \"This movie was amazing! I loved every minute of it.\",\n",
        "    \"Terrible film. Waste of time and money. Do not watch.\",\n",
        "    \"It was okay. Nothing special but not bad either.\",\n",
        "    \"Best movie I have ever seen! Absolutely brilliant!\",\n",
        "    \"Boring and slow. I fell asleep halfway through.\"\n",
        "]\n",
        "\n",
        "print(\"\\nğŸ”® Predictions on new reviews:\\n\")\n",
        "for review in test_reviews:\n",
        "    prob, sentiment = predict_sentiment(review)\n",
        "    print(f\"ğŸ“ \\\"{review[:50]}...\\\"\" if len(review) > 50 else f\"ğŸ“ \\\"{review}\\\"\")\n",
        "    print(f\"   â†’ {prob:.1%} confident â†’ {sentiment}\\n\")\n",
        "\n",
        "# Try your own!\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ’¡ TRY YOUR OWN:\")\n",
        "print(\"=\"*60)\n",
        "my_review = \"I really enjoyed this film, great acting and story\"\n",
        "prob, sentiment = predict_sentiment(my_review)\n",
        "print(f\"ğŸ“ \\\"{my_review}\\\"\")\n",
        "print(f\"   â†’ {prob:.1%} confident â†’ {sentiment}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "f6lkrtYfuAHr",
        "outputId": "9bbd49d2-eeda-4bff-c584-dacfe45a8325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "OVERFITTING DEMO: BIG MODEL, NO REGULARIZATION\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kyle-anthonyhay/Documents/CODE/Revature-Training/Ai-Engineering/December/venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:100: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "You tried to call `count_params` on layer 'sequential', but the layer isn't built. You can build it manually via: `layer.build(input_shape)`.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      9\u001b[39m model_overfit = keras.Sequential([\n\u001b[32m     10\u001b[39m     layers.Embedding(\u001b[32m5000\u001b[39m, \u001b[32m128\u001b[39m, input_length=\u001b[32m100\u001b[39m),\n\u001b[32m     11\u001b[39m     layers.LSTM(\u001b[32m128\u001b[39m),\n\u001b[32m     12\u001b[39m     layers.Dense(\u001b[32m64\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     13\u001b[39m     layers.Dense(\u001b[32m1\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33msigmoid\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m ])\n\u001b[32m     15\u001b[39m model_overfit.compile(optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m, loss=\u001b[33m'\u001b[39m\u001b[33mbinary_crossentropy\u001b[39m\u001b[33m'\u001b[39m, metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mğŸ“Š Parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel_overfit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m history_overfit = model_overfit.fit(\n\u001b[32m     20\u001b[39m     x_train, y_train,\n\u001b[32m     21\u001b[39m     epochs=\u001b[32m5\u001b[39m,\n\u001b[32m     22\u001b[39m     batch_size=\u001b[32m128\u001b[39m,\n\u001b[32m     23\u001b[39m     validation_split=\u001b[32m0.2\u001b[39m\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m train_acc = history_overfit.history[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CODE/Revature-Training/Ai-Engineering/December/venv/lib/python3.12/site-packages/keras/src/layers/layer.py:1465\u001b[39m, in \u001b[36mLayer.count_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1459\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Count the total number of scalars composing the weights.\u001b[39;00m\n\u001b[32m   1460\u001b[39m \n\u001b[32m   1461\u001b[39m \u001b[33;03mReturns:\u001b[39;00m\n\u001b[32m   1462\u001b[39m \u001b[33;03m    An integer count.\u001b[39;00m\n\u001b[32m   1463\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.built:\n\u001b[32m-> \u001b[39m\u001b[32m1465\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1466\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou tried to call `count_params` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1467\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mon layer \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1468\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut the layer isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt built. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1469\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can build it manually via: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1470\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`layer.build(input_shape)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1471\u001b[39m     )\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m summary_utils.count_params(\u001b[38;5;28mself\u001b[39m.weights)\n",
            "\u001b[31mValueError\u001b[39m: You tried to call `count_params` on layer 'sequential', but the layer isn't built. You can build it manually via: `layer.build(input_shape)`."
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 13: OVERFIT MODEL\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"OVERFITTING DEMO: BIG MODEL, NO REGULARIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model_overfit = keras.Sequential([\n",
        "    layers.Embedding(5000, 128, input_length=100),\n",
        "    layers.LSTM(128),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_overfit.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(f\"ğŸ“Š Parameters: {model_overfit.count_params():,}\\n\")\n",
        "\n",
        "history_overfit = model_overfit.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=5,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_acc = history_overfit.history['accuracy'][-1]\n",
        "val_acc = history_overfit.history['val_accuracy'][-1]\n",
        "gap = train_acc - val_acc\n",
        "\n",
        "print(f\"\\nğŸ“Š Train: {train_acc:.1%} | Val: {val_acc:.1%} | Gap: {gap:.1%}\")\n",
        "if gap > 0.1:\n",
        "    print(\"âš ï¸  Gap > 10% = OVERFITTING!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
