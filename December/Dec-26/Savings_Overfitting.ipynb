{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFuIIiUAGP9e",
        "outputId": "8037e793-f78a-4558-eae0-392f70a2d263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Libraries imported!\n",
            "\n",
            "âœ… Data loaded!\n",
            "   Training samples: 20000\n",
            "   Test samples: 5000\n",
            "   Sequence length: 100 words\n",
            "\n",
            "ğŸ“š Sample reviews:\n",
            "\n",
            "ğŸ“ Review #0 (POSITIVE ğŸ˜Š)\n",
            "--------------------------------------------------\n",
            "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNKNOWN> is an amazing actor and now the same being director <UNKNOWN> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNKNOWN> ...\n",
            "\n",
            "ğŸ“ Review #1 (NEGATIVE ğŸ˜)\n",
            "--------------------------------------------------\n",
            "<START> big hair big <UNKNOWN> bad music and a giant safety <UNKNOWN> these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an <UNKNOWN> the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are <UNKNOWN> and funny in equal <UNKNOWN> t...\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# MODEL PERSISTENCE & TRAINING OPTIMIZATION\n",
        "# =============================================================================\n",
        "# Colab notebook for teaching saving, checkpoints, early stopping, overfitting\n",
        "# Uses IMDB sentiment analysis throughout\n",
        "# =============================================================================\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 1: SETUP & LOAD DATA\n",
        "# =============================================================================\n",
        "\n",
        "!pip install -q tensorflow\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "print(\"âœ… Libraries imported!\")\n",
        "\n",
        "# Load IMDB dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=5000)\n",
        "\n",
        "# Use subset for faster training\n",
        "x_train, y_train = x_train[:20000], y_train[:20000]\n",
        "x_test, y_test = x_test[:5000], y_test[:5000]\n",
        "\n",
        "# Pad sequences using Keras\n",
        "x_train = keras.utils.pad_sequences(x_train, maxlen=100)\n",
        "x_test = keras.utils.pad_sequences(x_test, maxlen=100)\n",
        "\n",
        "# Create tokenizer from IMDB word_index\n",
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(num_words=5000)\n",
        "tokenizer.word_index = word_index\n",
        "\n",
        "# Helper to show reviews\n",
        "index_to_word = {idx + 3: word for word, idx in word_index.items()}\n",
        "index_to_word.update({0: '<PAD>', 1: '<START>', 2: '<UNKNOWN>'})\n",
        "\n",
        "def show_review(index):\n",
        "    \"\"\"Show a review in readable text.\"\"\"\n",
        "    # Get original sequence from dataset\n",
        "    (x_orig, y_orig), _ = keras.datasets.imdb.load_data(num_words=5000)\n",
        "    text = ' '.join([index_to_word.get(num, '?') for num in x_orig[index]])\n",
        "    sentiment = \"POSITIVE ğŸ˜Š\" if y_orig[index] == 1 else \"NEGATIVE ğŸ˜\"\n",
        "    print(f\"\\nğŸ“ Review #{index} ({sentiment})\")\n",
        "    print(\"-\" * 50)\n",
        "    print(text[:500] + \"...\" if len(text) > 500 else text)\n",
        "\n",
        "print(f\"\\nâœ… Data loaded!\")\n",
        "print(f\"   Training samples: {len(x_train)}\")\n",
        "print(f\"   Test samples: {len(x_test)}\")\n",
        "print(f\"   Sequence length: {x_train.shape[1]} words\")\n",
        "\n",
        "print(\"\\nğŸ“š Sample reviews:\")\n",
        "show_review(0)\n",
        "show_review(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht6tWYZ6GYoP",
        "outputId": "77ccaa94-e820-4810-b400-bb433b29b3a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=============================================================================\n",
            "THREE WAYS TO SAVE A MODEL\n",
            "=============================================================================\n",
            "\n",
            "1. Keras Format (.keras) - RECOMMENDED\n",
            "   â””â”€â”€ my_model.keras          â† Everything in one file\n",
            "\n",
            "   Use for: Default choice, full model save\n",
            "\n",
            "2. H5 Format (.h5) - LEGACY\n",
            "   â””â”€â”€ my_model.h5             â† Everything in one file\n",
            "\n",
            "   Use for: Older code compatibility\n",
            "\n",
            "3. Weights Only (.weights.h5)\n",
            "   â””â”€â”€ my_weights.weights.h5   â† Just the learned numbers\n",
            "\n",
            "   Use for: Transfer learning\n",
            "   âš ï¸  Must rebuild architecture before loading!\n",
            "\n",
            "4. Export for Serving\n",
            "   â””â”€â”€ my_model_export/        â† Folder for TFServing/TFLite\n",
            "\n",
            "   Use for: Production deployment\n",
            "\n",
            "=============================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 2: THREE FORMATS EXPLAINED\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\"\"\n",
        "=============================================================================\n",
        "THREE WAYS TO SAVE A MODEL\n",
        "=============================================================================\n",
        "\n",
        "1. Keras Format (.keras) - RECOMMENDED\n",
        "   â””â”€â”€ my_model.keras          â† Everything in one file\n",
        "\n",
        "   Use for: Default choice, full model save\n",
        "\n",
        "2. H5 Format (.h5) - LEGACY\n",
        "   â””â”€â”€ my_model.h5             â† Everything in one file\n",
        "\n",
        "   Use for: Older code compatibility\n",
        "\n",
        "3. Weights Only (.weights.h5)\n",
        "   â””â”€â”€ my_weights.weights.h5   â† Just the learned numbers\n",
        "\n",
        "   Use for: Transfer learning\n",
        "   âš ï¸  Must rebuild architecture before loading!\n",
        "\n",
        "4. Export for Serving\n",
        "   â””â”€â”€ my_model_export/        â† Folder for TFServing/TFLite\n",
        "\n",
        "   Use for: Production deployment\n",
        "\n",
        "=============================================================================\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "9qle99q9HLbm",
        "outputId": "4ca00392-9659-48f8-daba-3ad467a2bdc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TRAINING SENTIMENT MODEL\n",
            "============================================================\n",
            "\n",
            "ğŸ“Š Model Architecture:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kyle-anthonyhay/Documents/CODE/Revature-Training/Ai-Engineering/December/venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:100: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sentiment_model\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sentiment_model\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ output (\u001b[38;5;33mDense\u001b[0m)                  â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ‹ï¸ Training...\n",
            "Epoch 1/3\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.6816 - loss: 0.5957 - val_accuracy: 0.7853 - val_loss: 0.4726\n",
            "Epoch 2/3\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8296 - loss: 0.4026 - val_accuracy: 0.8295 - val_loss: 0.3814\n",
            "Epoch 3/3\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8572 - loss: 0.3435 - val_accuracy: 0.8400 - val_loss: 0.3769\n",
            "\n",
            "âœ… Training complete! Test accuracy: 81.22%\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 3: TRAIN MODEL\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING SENTIMENT MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Embedding(5000, 32, input_length=100, name='embedding'),\n",
        "    layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2, name='lstm'),\n",
        "    layers.Dense(1, activation='sigmoid', name='output')\n",
        "], name='sentiment_model')\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nğŸ“Š Model Architecture:\")\n",
        "model.summary()\n",
        "\n",
        "print(\"\\nğŸ‹ï¸ Training...\")\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=3,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"\\nâœ… Training complete! Test accuracy: {test_acc:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ta5cSiSHMLh",
        "outputId": "3cb0a212-7bfa-4b4e-949e-bc4a6800a047"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SAVING MODEL - THREE WAYS\n",
            "============================================================\n",
            "\n",
            "âœ… Keras format saved: saved_models/sentiment_model.keras\n",
            "âœ… H5 format saved: saved_models/sentiment_model.h5\n",
            "âœ… Weights saved: saved_models/sentiment_weights.weights.h5\n",
            "INFO:tensorflow:Assets written to: saved_models/sentiment_export/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_models/sentiment_export/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at 'saved_models/sentiment_export'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 100), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  4985454864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  4980531152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  4980539216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  4985456784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  4980539024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  4985456592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  4985457360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "âœ… Exported: saved_models/sentiment_export/\n",
            "\n",
            "ğŸ“ Files created:\n",
            "total 13336\n",
            "drwxr-xr-x@ 6 kyle-anthonyhay  staff   192B Dec 30 11:56 \u001b[34msentiment_export\u001b[m\u001b[m\n",
            "-rw-r--r--@ 1 kyle-anthonyhay  staff   2.0M Dec 30 11:56 sentiment_model.h5\n",
            "-rw-r--r--@ 1 kyle-anthonyhay  staff   2.0M Dec 30 11:56 sentiment_model.keras\n",
            "-rw-r--r--@ 1 kyle-anthonyhay  staff   1.9M Dec 30 11:56 sentiment_weights.weights.h5\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 4: SAVE MODEL - 3 WAYS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"SAVING MODEL - THREE WAYS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "os.makedirs('saved_models', exist_ok=True)\n",
        "\n",
        "# Method 1: Keras format (recommended)\n",
        "model.save('saved_models/sentiment_model.keras')\n",
        "print(\"\\nâœ… Keras format saved: saved_models/sentiment_model.keras\")\n",
        "\n",
        "# Method 2: H5 format (legacy)\n",
        "model.save('saved_models/sentiment_model.h5')\n",
        "print(\"âœ… H5 format saved: saved_models/sentiment_model.h5\")\n",
        "\n",
        "# Method 3: Weights only\n",
        "model.save_weights('saved_models/sentiment_weights.weights.h5')\n",
        "print(\"âœ… Weights saved: saved_models/sentiment_weights.weights.h5\")\n",
        "\n",
        "# Method 4: Export for TFServing/TFLite\n",
        "model.export('saved_models/sentiment_export')\n",
        "print(\"âœ… Exported: saved_models/sentiment_export/\")\n",
        "\n",
        "print(\"\\nğŸ“ Files created:\")\n",
        "!ls -lh saved_models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sICFigcmHcHX",
        "outputId": "eeb1f1bb-3d86-4382-9b41-a18305830038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "LOADING MODEL & PREDICTING NEW TEXT\n",
            "============================================================\n",
            "âœ… Model loaded!\n",
            "\n",
            "ğŸ”® Predictions:\n",
            "\n",
            "ğŸ“ \"This movie was amazing! I loved it.\"\n",
            "   â†’ 73.2% â†’ POSITIVE ğŸ˜Š\n",
            "\n",
            "ğŸ“ \"Terrible film. Waste of time.\"\n",
            "   â†’ 64.1% â†’ POSITIVE ğŸ˜Š\n",
            "\n",
            "ğŸ“ \"It was okay, nothing special.\"\n",
            "   â†’ 54.3% â†’ POSITIVE ğŸ˜Š\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 5: LOAD AND PREDICT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"LOADING MODEL & PREDICTING NEW TEXT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load model\n",
        "loaded_model = keras.models.load_model('saved_models/sentiment_model.keras')\n",
        "print(\"âœ… Model loaded!\")\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    \"\"\"Predict sentiment of any text.\"\"\"\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded = keras.utils.pad_sequences(sequence, maxlen=100)\n",
        "    prob = loaded_model.predict(padded, verbose=0)[0][0]\n",
        "    return prob, \"POSITIVE ğŸ˜Š\" if prob > 0.5 else \"NEGATIVE ğŸ˜\"\n",
        "\n",
        "# Test predictions\n",
        "test_reviews = [\n",
        "    \"This movie was amazing! I loved it.\",\n",
        "    \"Terrible film. Waste of time.\",\n",
        "    \"It was okay, nothing special.\",\n",
        "]\n",
        "\n",
        "print(\"\\nğŸ”® Predictions:\\n\")\n",
        "for review in test_reviews:\n",
        "    prob, sentiment = predict_sentiment(review)\n",
        "    print(f\"ğŸ“ \\\"{review}\\\"\")\n",
        "    print(f\"   â†’ {prob:.1%} â†’ {sentiment}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18l_b9Y5Hgtc",
        "outputId": "a36c779e-c4e5-4d08-97cd-1fe8aefe75f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SAVING TOKENIZER\n",
            "============================================================\n",
            "âœ… Tokenizer saved!\n",
            "âœ… Tokenizer loaded!\n",
            "\n",
            "ğŸ“– Test: \"great movie\" â†’ [[84, 17]]\n",
            "\n",
            "âš ï¸  Always save tokenizer with model!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 6: SAVE/LOAD TOKENIZER\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"SAVING TOKENIZER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Save tokenizer\n",
        "with open('saved_models/tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "print(\"âœ… Tokenizer saved!\")\n",
        "\n",
        "# Load tokenizer\n",
        "with open('saved_models/tokenizer.pkl', 'rb') as f:\n",
        "    loaded_tokenizer = pickle.load(f)\n",
        "print(\"âœ… Tokenizer loaded!\")\n",
        "\n",
        "# Test\n",
        "test_text = \"great movie\"\n",
        "print(f\"\\nğŸ“– Test: \\\"{test_text}\\\" â†’ {loaded_tokenizer.texts_to_sequences([test_text])}\")\n",
        "\n",
        "print(\"\\nâš ï¸  Always save tokenizer with model!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueDXUAEHHlbL",
        "outputId": "5c7b6ddb-1a3b-4942-b7b9-bf1360c0eeef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CHECKPOINT: SAVE BEST ONLY\n",
            "============================================================\n",
            "ğŸ‹ï¸ Training (watch 'saving model' messages)...\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kyle-anthonyhay/Documents/CODE/Revature-Training/Ai-Engineering/December/venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:100: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m123/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6143 - loss: 0.6592\n",
            "Epoch 1: val_accuracy improved from None to 0.78100, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 1: finished saving model to checkpoints/best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.6924 - loss: 0.5982 - val_accuracy: 0.7810 - val_loss: 0.4571\n",
            "Epoch 2/5\n",
            "\u001b[1m124/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8223 - loss: 0.4161\n",
            "Epoch 2: val_accuracy improved from 0.78100 to 0.83650, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 2: finished saving model to checkpoints/best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8268 - loss: 0.4049 - val_accuracy: 0.8365 - val_loss: 0.3734\n",
            "Epoch 3/5\n",
            "\u001b[1m124/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8549 - loss: 0.3475\n",
            "Epoch 3: val_accuracy did not improve from 0.83650\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8524 - loss: 0.3489 - val_accuracy: 0.8207 - val_loss: 0.4054\n",
            "Epoch 4/5\n",
            "\u001b[1m123/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8735 - loss: 0.3146\n",
            "Epoch 4: val_accuracy did not improve from 0.83650\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8679 - loss: 0.3267 - val_accuracy: 0.8148 - val_loss: 0.4210\n",
            "Epoch 5/5\n",
            "\u001b[1m123/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8825 - loss: 0.2975\n",
            "Epoch 5: val_accuracy did not improve from 0.83650\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8818 - loss: 0.2962 - val_accuracy: 0.8240 - val_loss: 0.4147\n",
            "\n",
            "âœ… Best model saved: checkpoints/best_model.keras\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 7: CHECKPOINT - SAVE BEST ONLY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CHECKPOINT: SAVE BEST ONLY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "\n",
        "model_ckpt = keras.Sequential([\n",
        "    layers.Embedding(5000, 32, input_length=100),\n",
        "    layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_ckpt.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "checkpoint_best = ModelCheckpoint(\n",
        "    'checkpoints/best_model.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"ğŸ‹ï¸ Training (watch 'saving model' messages)...\\n\")\n",
        "model_ckpt.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=5,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[checkpoint_best]\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Best model saved: checkpoints/best_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVKdBdD5HpOh",
        "outputId": "83b85ee9-e6ec-44ae-b346-2600ba9d9408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CHECKPOINT: SAVE EVERY EPOCH\n",
            "============================================================\n",
            "ğŸ‹ï¸ Training (saving every epoch)...\n",
            "\n",
            "Epoch 1/3\n",
            "\u001b[1m124/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6269 - loss: 0.6510\n",
            "Epoch 1: saving model to checkpoints/model_epoch01_acc0.811.keras\n",
            "\n",
            "Epoch 1: finished saving model to checkpoints/model_epoch01_acc0.811.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.7049 - loss: 0.5807 - val_accuracy: 0.8108 - val_loss: 0.4264\n",
            "Epoch 2/3\n",
            "\u001b[1m124/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8324 - loss: 0.3994\n",
            "Epoch 2: saving model to checkpoints/model_epoch02_acc0.818.keras\n",
            "\n",
            "Epoch 2: finished saving model to checkpoints/model_epoch02_acc0.818.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8347 - loss: 0.3921 - val_accuracy: 0.8183 - val_loss: 0.4031\n",
            "Epoch 3/3\n",
            "\u001b[1m123/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8621 - loss: 0.3354\n",
            "Epoch 3: saving model to checkpoints/model_epoch03_acc0.829.keras\n",
            "\n",
            "Epoch 3: finished saving model to checkpoints/model_epoch03_acc0.829.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8583 - loss: 0.3397 - val_accuracy: 0.8292 - val_loss: 0.3868\n",
            "\n",
            "ğŸ“ All checkpoints:\n",
            "-rw-r--r--@ 1 kyle-anthonyhay  staff  2048076 Dec 30 11:57 checkpoints/model_epoch01_acc0.811.keras\n",
            "-rw-r--r--@ 1 kyle-anthonyhay  staff  2048076 Dec 30 11:57 checkpoints/model_epoch02_acc0.818.keras\n",
            "-rw-r--r--@ 1 kyle-anthonyhay  staff  2048076 Dec 30 11:57 checkpoints/model_epoch03_acc0.829.keras\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 8: CHECKPOINT - SAVE EVERY EPOCH\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CHECKPOINT: SAVE EVERY EPOCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model_all = keras.Sequential([\n",
        "    layers.Embedding(5000, 32, input_length=100),\n",
        "    layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_all.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "checkpoint_all = ModelCheckpoint(\n",
        "    'checkpoints/model_epoch{epoch:02d}_acc{val_accuracy:.3f}.keras',\n",
        "    save_best_only=False,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"ğŸ‹ï¸ Training (saving every epoch)...\\n\")\n",
        "model_all.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=3,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[checkpoint_all]\n",
        ")\n",
        "\n",
        "print(\"\\nğŸ“ All checkpoints:\")\n",
        "!ls -la checkpoints/model_epoch*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACy32nKWH5SK",
        "outputId": "b1694fe7-b131-4720-882c-5fb362fbfe08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "RESUME INTERRUPTED TRAINING\n",
            "============================================================\n",
            "ğŸ“‚ Loading checkpoint...\n",
            "âœ… Loaded!\n",
            "\n",
            "ğŸ‹ï¸ Resuming from epoch 3 to 6...\n",
            "Epoch 4/6\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.8536 - loss: 0.3549 - val_accuracy: 0.8298 - val_loss: 0.3939\n",
            "Epoch 5/6\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8690 - loss: 0.3240 - val_accuracy: 0.8422 - val_loss: 0.3779\n",
            "Epoch 6/6\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8851 - loss: 0.2923 - val_accuracy: 0.8367 - val_loss: 0.3893\n",
            "\n",
            "âœ… Training resumed!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 9: RESUME TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RESUME INTERRUPTED TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"ğŸ“‚ Loading checkpoint...\")\n",
        "resumed_model = keras.models.load_model('checkpoints/best_model.keras')\n",
        "print(\"âœ… Loaded!\")\n",
        "\n",
        "print(\"\\nğŸ‹ï¸ Resuming from epoch 3 to 6...\")\n",
        "resumed_model.fit(\n",
        "    x_train, y_train,\n",
        "    initial_epoch=3,\n",
        "    epochs=6,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2\n",
        ")\n",
        "print(\"\\nâœ… Training resumed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXwvouwWIs3O",
        "outputId": "b9de0a94-bfd7-4cc7-cdf4-d1f57e7134dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "EARLY STOPPING\n",
            "============================================================\n",
            "ğŸ‹ï¸ Training with epochs=20 (early stopping will interrupt)...\n",
            "\n",
            "Epoch 1/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.6883 - loss: 0.5948 - val_accuracy: 0.8033 - val_loss: 0.4380\n",
            "Epoch 2/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8294 - loss: 0.3990 - val_accuracy: 0.8267 - val_loss: 0.3848\n",
            "Epoch 3/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8587 - loss: 0.3411 - val_accuracy: 0.8215 - val_loss: 0.3961\n",
            "Epoch 4/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8767 - loss: 0.3040 - val_accuracy: 0.8202 - val_loss: 0.3958\n",
            "Epoch 4: early stopping\n",
            "Restoring model weights from the end of the best epoch: 2.\n",
            "\n",
            "âœ… Stopped at epoch 4 (saved 16 epochs!)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 10: EARLY STOPPING\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"EARLY STOPPING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model_es = keras.Sequential([\n",
        "    layers.Embedding(5000, 32, input_length=100),\n",
        "    layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_es.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=2,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"ğŸ‹ï¸ Training with epochs=20 (early stopping will interrupt)...\\n\")\n",
        "history_es = model_es.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "actual_epochs = len(history_es.history['loss'])\n",
        "print(f\"\\nâœ… Stopped at epoch {actual_epochs} (saved {20 - actual_epochs} epochs!)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT4KtYPkI9DQ",
        "outputId": "77366912-9311-447a-f941-6e2b3959f45c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "REDUCE LEARNING RATE ON PLATEAU\n",
            "============================================================\n",
            "ğŸ‹ï¸ Training (watch 'reducing learning rate' messages)...\n",
            "\n",
            "Epoch 1/6\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.6929 - loss: 0.5865 - val_accuracy: 0.7940 - val_loss: 0.4470 - learning_rate: 0.0010\n",
            "Epoch 2/6\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8291 - loss: 0.3990 - val_accuracy: 0.8235 - val_loss: 0.4000 - learning_rate: 0.0010\n",
            "Epoch 3/6\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8589 - loss: 0.3451 - val_accuracy: 0.8393 - val_loss: 0.3774 - learning_rate: 0.0010\n",
            "Epoch 4/6\n",
            "\u001b[1m123/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8722 - loss: 0.3151\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8737 - loss: 0.3122 - val_accuracy: 0.8305 - val_loss: 0.3909 - learning_rate: 0.0010\n",
            "Epoch 5/6\n",
            "\u001b[1m124/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8989 - loss: 0.2700\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8952 - loss: 0.2756 - val_accuracy: 0.8313 - val_loss: 0.3892 - learning_rate: 5.0000e-04\n",
            "Epoch 6/6\n",
            "\u001b[1m124/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9028 - loss: 0.2514\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9036 - loss: 0.2531 - val_accuracy: 0.8382 - val_loss: 0.3880 - learning_rate: 2.5000e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x12a628b30>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 11: REDUCE LR ON PLATEAU\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"REDUCE LEARNING RATE ON PLATEAU\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model_lr = keras.Sequential([\n",
        "    layers.Embedding(5000, 32, input_length=100),\n",
        "    layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_lr.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=1,\n",
        "    min_lr=0.00001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"ğŸ‹ï¸ Training (watch 'reducing learning rate' messages)...\\n\")\n",
        "model_lr.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=6,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[reduce_lr]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNZNV-GHJhvr",
        "outputId": "3b48f456-917a-40e5-ee62-af2cc21025b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "PRODUCTION SETUP: ALL CALLBACKS\n",
            "============================================================\n",
            "ğŸ‹ï¸ Training with all callbacks...\n",
            "\n",
            "Epoch 1/15\n",
            "\u001b[1m124/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5901 - loss: 0.6597\n",
            "Epoch 1: val_accuracy improved from None to 0.79575, saving model to checkpoints/production_best.keras\n",
            "\n",
            "Epoch 1: finished saving model to checkpoints/production_best.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.6833 - loss: 0.5955 - val_accuracy: 0.7958 - val_loss: 0.4532 - learning_rate: 0.0010\n",
            "Epoch 2/15\n",
            "\u001b[1m124/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8286 - loss: 0.4089\n",
            "Epoch 2: val_accuracy improved from 0.79575 to 0.82175, saving model to checkpoints/production_best.keras\n",
            "\n",
            "Epoch 2: finished saving model to checkpoints/production_best.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8276 - loss: 0.4055 - val_accuracy: 0.8217 - val_loss: 0.3994 - learning_rate: 0.0010\n",
            "Epoch 3/15\n",
            "\u001b[1m124/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8553 - loss: 0.3506\n",
            "Epoch 3: val_accuracy improved from 0.82175 to 0.82750, saving model to checkpoints/production_best.keras\n",
            "\n",
            "Epoch 3: finished saving model to checkpoints/production_best.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8537 - loss: 0.3518 - val_accuracy: 0.8275 - val_loss: 0.3971 - learning_rate: 0.0010\n",
            "Epoch 4/15\n",
            "\u001b[1m123/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8787 - loss: 0.3116\n",
            "Epoch 4: val_accuracy improved from 0.82750 to 0.83125, saving model to checkpoints/production_best.keras\n",
            "\n",
            "Epoch 4: finished saving model to checkpoints/production_best.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8732 - loss: 0.3172 - val_accuracy: 0.8313 - val_loss: 0.3925 - learning_rate: 0.0010\n",
            "Epoch 5/15\n",
            "\u001b[1m124/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8845 - loss: 0.2916\n",
            "Epoch 5: val_accuracy improved from 0.83125 to 0.83950, saving model to checkpoints/production_best.keras\n",
            "\n",
            "Epoch 5: finished saving model to checkpoints/production_best.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8828 - loss: 0.2924 - val_accuracy: 0.8395 - val_loss: 0.3874 - learning_rate: 0.0010\n",
            "Epoch 6/15\n",
            "\u001b[1m124/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9000 - loss: 0.2627\n",
            "Epoch 6: val_accuracy did not improve from 0.83950\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8929 - loss: 0.2746 - val_accuracy: 0.8347 - val_loss: 0.3925 - learning_rate: 0.0010\n",
            "Epoch 7/15\n",
            "\u001b[1m124/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9102 - loss: 0.2409\n",
            "Epoch 7: val_accuracy did not improve from 0.83950\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9086 - loss: 0.2408 - val_accuracy: 0.8317 - val_loss: 0.4097 - learning_rate: 5.0000e-04\n",
            "Epoch 8/15\n",
            "\u001b[1m124/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9165 - loss: 0.2264\n",
            "Epoch 8: val_accuracy did not improve from 0.83950\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9162 - loss: 0.2270 - val_accuracy: 0.8305 - val_loss: 0.4109 - learning_rate: 2.5000e-04\n",
            "Epoch 8: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "\n",
            "âœ… Production training complete!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 12: ALL CALLBACKS TOGETHER\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PRODUCTION SETUP: ALL CALLBACKS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model_prod = keras.Sequential([\n",
        "    layers.Embedding(5000, 32, input_length=100),\n",
        "    layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_prod.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "callbacks_prod = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
        "    ModelCheckpoint('checkpoints/production_best.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, verbose=1)\n",
        "]\n",
        "\n",
        "print(\"ğŸ‹ï¸ Training with all callbacks...\\n\")\n",
        "model_prod.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=15,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    callbacks=callbacks_prod\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Production training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_TS9TYmKKhy",
        "outputId": "8c79d952-2c13-4580-82eb-9d4c43ba443f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "OVERFITTING DEMO: BIG MODEL, NO REGULARIZATION\n",
            "============================================================\n",
            "ğŸ“Š Parameters: 779,905\n",
            "\n",
            "Epoch 1/5\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.7449 - loss: 0.5004 - val_accuracy: 0.8370 - val_loss: 0.3660\n",
            "Epoch 2/5\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.8743 - loss: 0.3044 - val_accuracy: 0.8510 - val_loss: 0.3632\n",
            "Epoch 3/5\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 0.8964 - loss: 0.2550 - val_accuracy: 0.8440 - val_loss: 0.3700\n",
            "Epoch 4/5\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.9236 - loss: 0.1963 - val_accuracy: 0.8380 - val_loss: 0.4322\n",
            "Epoch 5/5\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 0.9426 - loss: 0.1494 - val_accuracy: 0.8410 - val_loss: 0.4811\n",
            "\n",
            "ğŸ“Š Train: 94.3% | Val: 84.1% | Gap: 10.2%\n",
            "âš ï¸  Gap > 10% = OVERFITTING!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 13: OVERFIT MODEL\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"OVERFITTING DEMO: BIG MODEL, NO REGULARIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model_overfit = keras.Sequential([\n",
        "    layers.Embedding(5000, 128, input_length=100),\n",
        "    layers.LSTM(128),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_overfit.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Build model first to count parameters\n",
        "model_overfit.build(input_shape=(None, 100))\n",
        "print(f\"ğŸ“Š Parameters: {model_overfit.count_params():,}\\n\")\n",
        "\n",
        "history_overfit = model_overfit.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=5,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_acc = history_overfit.history['accuracy'][-1]\n",
        "val_acc = history_overfit.history['val_accuracy'][-1]\n",
        "gap = train_acc - val_acc\n",
        "\n",
        "print(f\"\\nğŸ“Š Train: {train_acc:.1%} | Val: {val_acc:.1%} | Gap: {gap:.1%}\")\n",
        "if gap > 0.1:\n",
        "    print(\"âš ï¸  Gap > 10% = OVERFITTING!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0D_Ky2bK_zI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
