{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3770376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 04: Production Pipeline - Starter Code\n",
    "\n",
    "Build a complete production-ready training pipeline.\n",
    "\n",
    "This is a capstone exercise combining all Friday topics.\n",
    "\n",
    "Prerequisites:\n",
    "- All Friday readings and demos\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e78872cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRODUCTION TRAINER CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class ProductionTrainer:\n",
    "    \"\"\"\n",
    "    Production-ready training pipeline.\n",
    "    \n",
    "    FEATURES TO IMPLEMENT:\n",
    "    1. Automatic checkpointing (save best model)\n",
    "    2. Early stopping (prevent overfitting)\n",
    "    3. TensorBoard logging (visualization)\n",
    "    4. Model versioning (timestamped runs)\n",
    "    5. Configuration saving (reproducibility)\n",
    "    6. Regularization (dropout + L2)\n",
    "    \n",
    "    DIRECTORY STRUCTURE:\n",
    "    production_runs/\n",
    "      modelname_20240115_143000/\n",
    "        checkpoints/\n",
    "          best_model.keras\n",
    "        logs/\n",
    "          train/\n",
    "          validation/\n",
    "        models/\n",
    "          final_model.keras\n",
    "        config.json\n",
    "        summary.json\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, output_dir='production_runs'):\n",
    "        self.model_name = model_name\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Versioned run directory\n",
    "        self.timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        self.run_dir = os.path.join(output_dir, f'{model_name}_{self.timestamp}')\n",
    "        \n",
    "        # Subdirectories\n",
    "        self.checkpoint_dir = os.path.join(self.run_dir, 'checkpoints')\n",
    "        self.log_dir = os.path.join(self.run_dir, 'logs')\n",
    "        self.model_dir = os.path.join(self.run_dir, 'models')\n",
    "        \n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.config = {}\n",
    "    \n",
    "    def _setup_directories(self):\n",
    "        \"\"\"\n",
    "        Create all output directories.\n",
    "        Use os.makedirs(path, exist_ok=True)\n",
    "        \"\"\"\n",
    "        os.makedirs(self.run_dir, exist_ok=True)\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "    \n",
    "    def build_model(self, input_shape, num_classes, \n",
    "                    hidden_layers=[128, 64], \n",
    "                    dropout_rate=0.3, \n",
    "                    l2_lambda=0.001):\n",
    "        \"\"\"\n",
    "        Build model with regularization.\n",
    "        \n",
    "        ARCHITECTURE:\n",
    "        Input -> [Dense + Dropout] x N -> Output\n",
    "        \n",
    "        STORE CONFIG for reproducibility:\n",
    "        self.config = {\n",
    "            'input_shape': input_shape,\n",
    "            'num_classes': num_classes,\n",
    "            'hidden_layers': hidden_layers,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'l2_lambda': l2_lambda\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.config = {\n",
    "            'input_shape': input_shape,\n",
    "            'num_classes': num_classes,\n",
    "            'hidden_layers': hidden_layers,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'l2_lambda': l2_lambda\n",
    "        }\n",
    "        \n",
    "        self.model = keras.Sequential()\n",
    "        self.model.add(layers.Input(shape=input_shape))\n",
    "        \n",
    "        for units in hidden_layers:\n",
    "            self.model.add(layers.Dense(\n",
    "                units, \n",
    "                activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(l2_lambda)\n",
    "            ))\n",
    "            self.model.add(layers.Dropout(dropout_rate))\n",
    "        \n",
    "        self.model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def _create_callbacks(self, patience=10):\n",
    "        \"\"\"\n",
    "        Create production callbacks.\n",
    "        \n",
    "        CALLBACKS TO CREATE:\n",
    "        1. ModelCheckpoint - save best model to checkpoint_dir\n",
    "        2. EarlyStopping - patience epochs, restore_best_weights=True\n",
    "        3. TensorBoard - log to log_dir\n",
    "        4. CSVLogger - save metrics to CSV\n",
    "        5. ReduceLROnPlateau - reduce LR when stuck\n",
    "        \n",
    "        SEE: demo_02_checkpoint_callback.py for ModelCheckpoint\n",
    "        SEE: demo_03_early_stopping.py for EarlyStopping\n",
    "        \"\"\"\n",
    "        callbacks = []\n",
    "        \n",
    "        checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(self.checkpoint_dir, 'best_model.keras'),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks.append(checkpoint)\n",
    "        \n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks.append(early_stopping)\n",
    "        \n",
    "        tensorboard = keras.callbacks.TensorBoard(\n",
    "            log_dir=self.log_dir,\n",
    "            histogram_freq=1\n",
    "        )\n",
    "        callbacks.append(tensorboard)\n",
    "        \n",
    "        csv_logger = keras.callbacks.CSVLogger(\n",
    "            os.path.join(self.run_dir, 'training_log.csv')\n",
    "        )\n",
    "        callbacks.append(csv_logger)\n",
    "        \n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=patience // 2,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks.append(reduce_lr)\n",
    "        \n",
    "        return callbacks\n",
    "    \n",
    "    def train(self, x_train, y_train, x_val, y_val,\n",
    "              epochs=100, batch_size=32, patience=10):\n",
    "        \"\"\"\n",
    "        Run training with all callbacks.\n",
    "        \n",
    "        STEPS:\n",
    "        1. Setup directories\n",
    "        2. Create callbacks\n",
    "        3. Train model\n",
    "        4. Save final model\n",
    "        5. Save config and summary\n",
    "        \"\"\"\n",
    "        self._setup_directories()\n",
    "        \n",
    "        callbacks = self._create_callbacks(patience=patience)\n",
    "        \n",
    "        self.config.update({\n",
    "            'epochs': epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'patience': patience\n",
    "        })\n",
    "        \n",
    "        self.history = self.model.fit(\n",
    "            x_train, y_train,\n",
    "            validation_data=(x_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        self.model.save(os.path.join(self.model_dir, 'final_model.keras'))\n",
    "        \n",
    "        self._save_config()\n",
    "        self._save_summary()\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def _save_config(self):\n",
    "        \"\"\"\n",
    "        Save config.json with all hyperparameters.\n",
    "        Use json.dump(self.config, f, indent=2)\n",
    "        \"\"\"\n",
    "        config_path = os.path.join(self.run_dir, 'config.json')\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(self.config, f, indent=2)\n",
    "    \n",
    "    def _save_summary(self):\n",
    "        \"\"\"\n",
    "        Save summary.json with training results.\n",
    "        \n",
    "        INCLUDE:\n",
    "        - Best val_accuracy\n",
    "        - Best val_loss\n",
    "        - Total epochs trained\n",
    "        - Early stopped epoch (if applicable)\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            'best_val_accuracy': float(max(self.history.history['val_accuracy'])),\n",
    "            'best_val_loss': float(min(self.history.history['val_loss'])),\n",
    "            'total_epochs_trained': len(self.history.history['loss']),\n",
    "            'configured_epochs': self.config.get('epochs', None),\n",
    "            'early_stopped': len(self.history.history['loss']) < self.config.get('epochs', 0)\n",
    "        }\n",
    "        \n",
    "        summary_path = os.path.join(self.run_dir, 'summary.json')\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "    \n",
    "    def load_best_model(self):\n",
    "        \"\"\"\n",
    "        Load the best checkpoint.\n",
    "        Return: loaded model\n",
    "        \"\"\"\n",
    "        best_model_path = os.path.join(self.checkpoint_dir, 'best_model.keras')\n",
    "        return keras.models.load_model(best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c4bfa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TEST THE PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def test_production_pipeline():\n",
    "    \"\"\"\n",
    "    Test the complete pipeline with MNIST.\n",
    "    \n",
    "    STEPS:\n",
    "    1. Load MNIST data\n",
    "    2. Create ProductionTrainer\n",
    "    3. Build model\n",
    "    4. Train\n",
    "    5. Load best model and evaluate on test set\n",
    "    6. Print summary of created files\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Testing Production Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load data\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
    "    x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
    "    \n",
    "    # Use subset for faster testing\n",
    "    x_train, y_train = x_train[:5000], y_train[:5000]\n",
    "    \n",
    "    # Validation split\n",
    "    x_val, y_val = x_train[-1000:], y_train[-1000:]\n",
    "    x_train, y_train = x_train[:-1000], y_train[:-1000]\n",
    "    \n",
    "    # YOUR CODE:\n",
    "    # 1. Create trainer\n",
    "    trainer = ProductionTrainer('mnist_classifier')\n",
    "    \n",
    "    # 2. Build model\n",
    "    trainer.build_model(input_shape=(784,), num_classes=10)\n",
    "    \n",
    "    # 3. Train\n",
    "    trainer.train(x_train, y_train, x_val, y_val, epochs=50)\n",
    "    \n",
    "    # 4. Load best and evaluate\n",
    "    best_model = trainer.load_best_model()\n",
    "    test_loss, test_acc = best_model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # 5. List created files\n",
    "    print(\"\\\\nCreated files:\")\n",
    "    for root, dirs, files in os.walk(trainer.run_dir):\n",
    "        for f in files:\n",
    "            print(f\"  {os.path.join(root, f)}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c49fd0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Exercise 04: Production Pipeline\n",
      "============================================================\n",
      "============================================================\n",
      "Testing Production Pipeline\n",
      "============================================================\n",
      "Epoch 1/50\n",
      "\u001b[1m 66/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3332 - loss: 2.2070  \n",
      "Epoch 1: val_loss improved from None to 0.67808, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 1: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6195 - loss: 1.4081 - val_accuracy: 0.8760 - val_loss: 0.6781 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m 66/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 771us/step - accuracy: 0.8169 - loss: 0.8116\n",
      "Epoch 2: val_loss improved from 0.67808 to 0.58540, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 2: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8367 - loss: 0.7501 - val_accuracy: 0.8810 - val_loss: 0.5854 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m 68/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 750us/step - accuracy: 0.8848 - loss: 0.6220\n",
      "Epoch 3: val_loss improved from 0.58540 to 0.49438, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 3: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8880 - loss: 0.5950 - val_accuracy: 0.9120 - val_loss: 0.4944 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 826us/step - accuracy: 0.9058 - loss: 0.5212\n",
      "Epoch 4: val_loss improved from 0.49438 to 0.47655, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 4: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9022 - loss: 0.5284 - val_accuracy: 0.9170 - val_loss: 0.4765 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - accuracy: 0.9185 - loss: 0.4845\n",
      "Epoch 5: val_loss improved from 0.47655 to 0.44441, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 5: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9165 - loss: 0.4779 - val_accuracy: 0.9270 - val_loss: 0.4444 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m 62/125\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 831us/step - accuracy: 0.9345 - loss: 0.4161\n",
      "Epoch 6: val_loss improved from 0.44441 to 0.44235, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 6: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9240 - loss: 0.4360 - val_accuracy: 0.9280 - val_loss: 0.4424 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m 68/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 749us/step - accuracy: 0.9297 - loss: 0.4098\n",
      "Epoch 7: val_loss improved from 0.44235 to 0.43942, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 7: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9370 - loss: 0.4040 - val_accuracy: 0.9290 - val_loss: 0.4394 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.9417 - loss: 0.3654\n",
      "Epoch 8: val_loss improved from 0.43942 to 0.40479, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 8: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9405 - loss: 0.3838 - val_accuracy: 0.9320 - val_loss: 0.4048 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 831us/step - accuracy: 0.9499 - loss: 0.3662\n",
      "Epoch 9: val_loss did not improve from 0.40479\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9513 - loss: 0.3642 - val_accuracy: 0.9310 - val_loss: 0.4218 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m118/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 859us/step - accuracy: 0.9479 - loss: 0.3415\n",
      "Epoch 10: val_loss improved from 0.40479 to 0.40111, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 10: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9473 - loss: 0.3477 - val_accuracy: 0.9310 - val_loss: 0.4011 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - accuracy: 0.9524 - loss: 0.3428\n",
      "Epoch 11: val_loss improved from 0.40111 to 0.39745, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 11: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9525 - loss: 0.3331 - val_accuracy: 0.9360 - val_loss: 0.3974 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m 61/125\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 843us/step - accuracy: 0.9593 - loss: 0.3056\n",
      "Epoch 12: val_loss improved from 0.39745 to 0.37277, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 12: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9570 - loss: 0.3196 - val_accuracy: 0.9410 - val_loss: 0.3728 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m 69/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 738us/step - accuracy: 0.9623 - loss: 0.2890\n",
      "Epoch 13: val_loss did not improve from 0.37277\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9610 - loss: 0.2977 - val_accuracy: 0.9340 - val_loss: 0.3918 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m 71/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 720us/step - accuracy: 0.9561 - loss: 0.2981\n",
      "Epoch 14: val_loss did not improve from 0.37277\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9592 - loss: 0.2947 - val_accuracy: 0.9350 - val_loss: 0.3863 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m 64/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 803us/step - accuracy: 0.9638 - loss: 0.2774\n",
      "Epoch 15: val_loss did not improve from 0.37277\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9617 - loss: 0.2910 - val_accuracy: 0.9370 - val_loss: 0.3889 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m 65/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 783us/step - accuracy: 0.9772 - loss: 0.2607\n",
      "Epoch 16: val_loss improved from 0.37277 to 0.37115, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 16: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9688 - loss: 0.2771 - val_accuracy: 0.9380 - val_loss: 0.3712 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m 63/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 815us/step - accuracy: 0.9750 - loss: 0.2481\n",
      "Epoch 17: val_loss did not improve from 0.37115\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9670 - loss: 0.2705 - val_accuracy: 0.9390 - val_loss: 0.3814 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m 66/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 777us/step - accuracy: 0.9662 - loss: 0.2638\n",
      "Epoch 18: val_loss did not improve from 0.37115\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9678 - loss: 0.2632 - val_accuracy: 0.9380 - val_loss: 0.3736 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m 69/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 746us/step - accuracy: 0.9724 - loss: 0.2537\n",
      "Epoch 19: val_loss improved from 0.37115 to 0.36119, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 19: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9645 - loss: 0.2670 - val_accuracy: 0.9420 - val_loss: 0.3612 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m 68/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 747us/step - accuracy: 0.9737 - loss: 0.2435\n",
      "Epoch 20: val_loss did not improve from 0.36119\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9728 - loss: 0.2531 - val_accuracy: 0.9400 - val_loss: 0.3862 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m 70/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 730us/step - accuracy: 0.9754 - loss: 0.2442\n",
      "Epoch 21: val_loss improved from 0.36119 to 0.35691, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 21: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9710 - loss: 0.2434 - val_accuracy: 0.9410 - val_loss: 0.3569 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m 71/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 722us/step - accuracy: 0.9788 - loss: 0.2202\n",
      "Epoch 22: val_loss did not improve from 0.35691\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9755 - loss: 0.2339 - val_accuracy: 0.9350 - val_loss: 0.3675 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m 72/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 713us/step - accuracy: 0.9799 - loss: 0.2275\n",
      "Epoch 23: val_loss did not improve from 0.35691\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step - accuracy: 0.9753 - loss: 0.2407 - val_accuracy: 0.9360 - val_loss: 0.3763 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m 73/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 701us/step - accuracy: 0.9667 - loss: 0.2405\n",
      "Epoch 24: val_loss improved from 0.35691 to 0.35001, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 24: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9695 - loss: 0.2363 - val_accuracy: 0.9380 - val_loss: 0.3500 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m 74/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 693us/step - accuracy: 0.9754 - loss: 0.2327\n",
      "Epoch 25: val_loss did not improve from 0.35001\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968us/step - accuracy: 0.9747 - loss: 0.2304 - val_accuracy: 0.9430 - val_loss: 0.3531 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m 75/125\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 684us/step - accuracy: 0.9773 - loss: 0.2186\n",
      "Epoch 26: val_loss did not improve from 0.35001\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step - accuracy: 0.9722 - loss: 0.2313 - val_accuracy: 0.9400 - val_loss: 0.3577 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m 71/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 715us/step - accuracy: 0.9698 - loss: 0.2454\n",
      "Epoch 27: val_loss did not improve from 0.35001\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step - accuracy: 0.9760 - loss: 0.2221 - val_accuracy: 0.9420 - val_loss: 0.3539 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m 75/125\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 684us/step - accuracy: 0.9817 - loss: 0.2120\n",
      "Epoch 28: val_loss did not improve from 0.35001\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - accuracy: 0.9830 - loss: 0.2153 - val_accuracy: 0.9360 - val_loss: 0.3698 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m 74/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 693us/step - accuracy: 0.9813 - loss: 0.2003\n",
      "Epoch 29: val_loss did not improve from 0.35001\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - accuracy: 0.9770 - loss: 0.2192 - val_accuracy: 0.9380 - val_loss: 0.3589 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m 74/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 687us/step - accuracy: 0.9820 - loss: 0.2146\n",
      "Epoch 30: val_loss improved from 0.35001 to 0.32858, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 30: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9820 - loss: 0.2060 - val_accuracy: 0.9440 - val_loss: 0.3286 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m 73/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 697us/step - accuracy: 0.9879 - loss: 0.1865\n",
      "Epoch 31: val_loss did not improve from 0.32858\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968us/step - accuracy: 0.9860 - loss: 0.1891 - val_accuracy: 0.9450 - val_loss: 0.3320 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m 73/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 703us/step - accuracy: 0.9870 - loss: 0.1804\n",
      "Epoch 32: val_loss improved from 0.32858 to 0.32467, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 32: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9862 - loss: 0.1823 - val_accuracy: 0.9440 - val_loss: 0.3247 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m 74/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 690us/step - accuracy: 0.9841 - loss: 0.1908\n",
      "Epoch 33: val_loss did not improve from 0.32467\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9852 - loss: 0.1825 - val_accuracy: 0.9450 - val_loss: 0.3262 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m100/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9911 - loss: 0.1710\n",
      "Epoch 34: val_loss improved from 0.32467 to 0.32222, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 34: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9877 - loss: 0.1733 - val_accuracy: 0.9450 - val_loss: 0.3222 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m 65/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 785us/step - accuracy: 0.9905 - loss: 0.1689\n",
      "Epoch 35: val_loss improved from 0.32222 to 0.31477, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 35: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9868 - loss: 0.1757 - val_accuracy: 0.9460 - val_loss: 0.3148 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m 65/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 783us/step - accuracy: 0.9905 - loss: 0.1721\n",
      "Epoch 36: val_loss improved from 0.31477 to 0.31232, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 36: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9895 - loss: 0.1693 - val_accuracy: 0.9460 - val_loss: 0.3123 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m 68/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 747us/step - accuracy: 0.9895 - loss: 0.1707\n",
      "Epoch 37: val_loss did not improve from 0.31232\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9893 - loss: 0.1673 - val_accuracy: 0.9430 - val_loss: 0.3143 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m 71/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 722us/step - accuracy: 0.9899 - loss: 0.1668\n",
      "Epoch 38: val_loss improved from 0.31232 to 0.31054, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 38: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9870 - loss: 0.1691 - val_accuracy: 0.9420 - val_loss: 0.3105 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m 72/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 706us/step - accuracy: 0.9807 - loss: 0.1800\n",
      "Epoch 39: val_loss improved from 0.31054 to 0.30819, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 39: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9847 - loss: 0.1735 - val_accuracy: 0.9510 - val_loss: 0.3082 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m 73/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 698us/step - accuracy: 0.9856 - loss: 0.1594\n",
      "Epoch 40: val_loss improved from 0.30819 to 0.30620, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 40: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9855 - loss: 0.1696 - val_accuracy: 0.9480 - val_loss: 0.3062 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m 73/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 699us/step - accuracy: 0.9901 - loss: 0.1565\n",
      "Epoch 41: val_loss did not improve from 0.30620\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step - accuracy: 0.9880 - loss: 0.1622 - val_accuracy: 0.9380 - val_loss: 0.3140 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m 73/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 695us/step - accuracy: 0.9921 - loss: 0.1535\n",
      "Epoch 42: val_loss did not improve from 0.30620\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - accuracy: 0.9900 - loss: 0.1554 - val_accuracy: 0.9430 - val_loss: 0.3180 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m 70/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 729us/step - accuracy: 0.9834 - loss: 0.1629\n",
      "Epoch 43: val_loss improved from 0.30620 to 0.30199, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 43: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9825 - loss: 0.1662 - val_accuracy: 0.9440 - val_loss: 0.3020 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m 74/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 690us/step - accuracy: 0.9923 - loss: 0.1542\n",
      "Epoch 44: val_loss did not improve from 0.30199\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step - accuracy: 0.9900 - loss: 0.1585 - val_accuracy: 0.9430 - val_loss: 0.3056 - learning_rate: 5.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m 74/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 695us/step - accuracy: 0.9912 - loss: 0.1496\n",
      "Epoch 45: val_loss improved from 0.30199 to 0.29218, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 45: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9875 - loss: 0.1571 - val_accuracy: 0.9440 - val_loss: 0.2922 - learning_rate: 5.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m 71/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 718us/step - accuracy: 0.9895 - loss: 0.1506\n",
      "Epoch 46: val_loss did not improve from 0.29218\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step - accuracy: 0.9875 - loss: 0.1557 - val_accuracy: 0.9470 - val_loss: 0.3076 - learning_rate: 5.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m 74/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 691us/step - accuracy: 0.9881 - loss: 0.1533\n",
      "Epoch 47: val_loss did not improve from 0.29218\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - accuracy: 0.9833 - loss: 0.1633 - val_accuracy: 0.9500 - val_loss: 0.2962 - learning_rate: 5.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m 74/125\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 690us/step - accuracy: 0.9926 - loss: 0.1434\n",
      "Epoch 48: val_loss did not improve from 0.29218\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9877 - loss: 0.1513 - val_accuracy: 0.9440 - val_loss: 0.2980 - learning_rate: 5.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m 66/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 773us/step - accuracy: 0.9880 - loss: 0.1489\n",
      "Epoch 49: val_loss improved from 0.29218 to 0.28887, saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\n",
      "Epoch 49: finished saving model to production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9860 - loss: 0.1506 - val_accuracy: 0.9480 - val_loss: 0.2889 - learning_rate: 5.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m 66/125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 772us/step - accuracy: 0.9849 - loss: 0.1621\n",
      "Epoch 50: val_loss did not improve from 0.28887\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9870 - loss: 0.1546 - val_accuracy: 0.9500 - val_loss: 0.2984 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353us/step - accuracy: 0.9425 - loss: 0.3016\n",
      "Test accuracy: 0.9425\n",
      "\\nCreated files:\n",
      "  production_runs/mnist_classifier_20251227_213843/summary.json\n",
      "  production_runs/mnist_classifier_20251227_213843/config.json\n",
      "  production_runs/mnist_classifier_20251227_213843/training_log.csv\n",
      "  production_runs/mnist_classifier_20251227_213843/checkpoints/best_model.keras\n",
      "  production_runs/mnist_classifier_20251227_213843/models/final_model.keras\n",
      "  production_runs/mnist_classifier_20251227_213843/logs/train/events.out.tfevents.1766889523.Mac.22749.2.v2\n",
      "  production_runs/mnist_classifier_20251227_213843/logs/validation/events.out.tfevents.1766889523.Mac.22749.3.v2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Exercise 04: Production Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Uncomment to test:\n",
    "    test_production_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
