{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 03: Embedding Exploration - Starter Code\n",
    "\n",
    "Experiment with word embeddings and analyze semantic relationships.\n",
    "\n",
    "Prerequisites:\n",
    "- Reading: 04-introduction-to-embeddings.md\n",
    "- Reading: 05-word2vec-intuition.md\n",
    "- Demo: demo_03_word2vec_visualization.py (REFERENCE FOR VISUALIZATION)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from unittest.case import enterModuleContext\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 3.1: Keras Embedding Basics\n",
    "# ============================================================================\n",
    "\n",
    "def explore_embedding_layer():\n",
    "    \"\"\"\n",
    "    Understand the Keras Embedding layer.\n",
    "    \n",
    "    KEY CONCEPT:\n",
    "    - Embedding(vocab_size, embed_dim) creates a lookup table\n",
    "    - Input: integer indices (word IDs)\n",
    "    - Output: dense vectors of size embed_dim\n",
    "    \n",
    "    SHAPE TRANSFORMATION:\n",
    "    - Input: (batch_size, sequence_length) - integers\n",
    "    - Output: (batch_size, sequence_length, embed_dim) - floats\n",
    "    \n",
    "    PARAMETERS:\n",
    "    - Total params = vocab_size * embed_dim\n",
    "    \n",
    "    TEST THESE EMBEDDING DIMS: 16, 32, 64, 128, 256\n",
    "    with vocab_size = 1000\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Task 3.1: Embedding Layer Basics\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    vocab_size = 1000\n",
    "    embedding_dims = [16, 32, 64, 128, 256]\n",
    "    \n",
    "    print(f\"Vocabulary Size: {vocab_size}\")\n",
    "    print(f\"\\n{'Embedding Dim':<15} {'Parameters':<15} {'Memory (MB)':<15}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    sample_input = np.array([[1, 2, 3, 4]])\n",
    "    \n",
    "    for embed_dim in embedding_dims:\n",
    "        embedding_layer = layers.Embedding(vocab_size, embed_dim)\n",
    "        params = vocab_size * embed_dim\n",
    "        memory_mb = (params * 4) / (1024 * 1024)\n",
    "        \n",
    "        print(f\"{embed_dim:<15} {params:<15,} {memory_mb:<15.4f}\")\n",
    "        \n",
    "        output = embedding_layer(sample_input)\n",
    "        print(f\"  Input shape: {sample_input.shape}\")\n",
    "        print(f\"  Output shape: {output.shape}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Key Observations:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. Parameters = vocab_size × embedding_dim\")\n",
    "    print(\"2. Memory = parameters × 4 bytes (float32)\")\n",
    "    print(\"3. Output shape adds embedding_dim to input shape\")\n",
    "    print(\"4. Higher dimensions = more capacity but more memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2818daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# TASK 3.2: Training Embeddings\n",
    "# ============================================================================\n",
    "\n",
    "def train_sentiment_classifier():\n",
    "    \"\"\"\n",
    "    Train embeddings through a simple sentiment task.\n",
    "    \n",
    "    SAMPLE DATA (provided):\n",
    "    - Positive: \"love\", \"great\", \"amazing\", \"wonderful\"\n",
    "    - Negative: \"terrible\", \"awful\", \"hate\", \"bad\"\n",
    "    \n",
    "    MODEL ARCHITECTURE:\n",
    "    1. Embedding(vocab_size, 16)\n",
    "    2. GlobalAveragePooling1D()  # Average all word vectors\n",
    "    3. Dense(1, sigmoid)\n",
    "    \n",
    "    AFTER TRAINING:\n",
    "    - Extract embedding weights: model.layers[0].get_weights()[0]\n",
    "    - Words with similar sentiment should have similar vectors\n",
    "    \n",
    "    SEE: demo_03_word2vec_visualization.py for embedding extraction\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Task 3.2: Training Embeddings\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sample data\n",
    "    texts = [\n",
    "        \"I love this movie it is amazing\",\n",
    "        \"Great film excellent acting wonderful\",\n",
    "        \"This movie is beautiful and amazing\",\n",
    "        \"Terrible movie waste of time awful\",\n",
    "        \"Bad film I hated it horrible\",\n",
    "        \"Disappointing and boring terrible waste\"\n",
    "    ]\n",
    "    labels = [1, 1, 1, 0, 0, 0]  # 1=positive, 0=negative\n",
    "    \n",
    "    # 1. Tokenize texts\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    word_to_idx = tokenizer.word_index\n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    \n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    print(f\"Word to index mapping: {word_to_idx}\")\n",
    "    print(f\"\\nSample sequences: {sequences[:2]}\")\n",
    "    \n",
    "    # 2. Pad sequences to same length\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "        sequences, maxlen=max_length, padding='post'\n",
    "    )\n",
    "    \n",
    "    X = np.array(padded_sequences)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    print(f\"\\nPadded sequences shape: {X.shape}\")\n",
    "    print(f\"Labels shape: {y.shape}\")\n",
    "    \n",
    "    # 3. Build model\n",
    "    model = keras.Sequential([\n",
    "        layers.Embedding(vocab_size, 16, input_length=max_length),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # 4. Train model\n",
    "    print(\"\\nTraining model...\")\n",
    "    history = model.fit(\n",
    "        X, y,\n",
    "        epochs=100,\n",
    "        verbose=0,\n",
    "        validation_split=0.0\n",
    "    )\n",
    "    \n",
    "    final_loss = history.history['loss'][-1]\n",
    "    final_acc = history.history['accuracy'][-1]\n",
    "    print(f\"Final training loss: {final_loss:.4f}\")\n",
    "    print(f\"Final training accuracy: {final_acc:.4f}\")\n",
    "    \n",
    "    # 5. Extract embedding matrix\n",
    "    embedding_layer = model.layers[0]\n",
    "    embeddings = embedding_layer.get_weights()[0]\n",
    "    \n",
    "    print(f\"\\nEmbedding matrix shape: {embeddings.shape}\")\n",
    "    print(f\"Embeddings for first 5 words:\")\n",
    "    for i in range(1, min(6, vocab_size)):\n",
    "        word = idx_to_word.get(i, f\"<word_{i}>\")\n",
    "        print(f\"  '{word}': {embeddings[i][:5]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return embeddings, word_to_idx, idx_to_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aebe191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# TASK 3.3: Semantic Similarity\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_semantic_similarity(embeddings, word_to_idx):\n",
    "    \"\"\"\n",
    "    Analyze semantic relationships in learned embeddings.\n",
    "    \n",
    "    COSINE SIMILARITY FORMULA:\n",
    "    cos_sim(a, b) = (a . b) / (||a|| * ||b||)\n",
    "    \n",
    "    TEST THESE PAIRS:\n",
    "    - (\"love\", \"amazing\") - should be similar (both positive)\n",
    "    - (\"terrible\", \"awful\") - should be similar (both negative)\n",
    "    - (\"love\", \"hate\") - should be dissimilar (opposite sentiment)\n",
    "    \n",
    "    COMPARE TO ONE-HOT:\n",
    "    - One-hot similarity was always 0 between different words\n",
    "    - Embeddings should capture meaning!\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Task 3.3: Semantic Similarity\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    test_pairs = [\n",
    "        (\"love\", \"amazing\", \"Both positive - should be similar\"),\n",
    "        (\"terrible\", \"awful\", \"Both negative - should be similar\"),\n",
    "        (\"love\", \"hate\", \"Opposite sentiment - should be dissimilar\")\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nCosine Similarity Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for word1, word2, description in test_pairs:\n",
    "        if word1 not in word_to_idx:\n",
    "            print(f\"'{word1}' not in vocabulary\")\n",
    "            continue\n",
    "        if word2 not in word_to_idx:\n",
    "            print(f\"'{word2}' not in vocabulary\")\n",
    "            continue\n",
    "        \n",
    "        idx1 = word_to_idx[word1]\n",
    "        idx2 = word_to_idx[word2]\n",
    "        \n",
    "        vec1 = embeddings[idx1]\n",
    "        vec2 = embeddings[idx2]\n",
    "        \n",
    "        sim = cosine_similarity(vec1, vec2)\n",
    "        results.append((word1, word2, sim, description))\n",
    "        \n",
    "        print(f\"\\nPair: '{word1}' vs '{word2}'\")\n",
    "        print(f\"  Description: {description}\")\n",
    "        print(f\"  Cosine Similarity: {sim:.4f}\")\n",
    "        print(f\"  Interpretation: \", end=\"\")\n",
    "        \n",
    "        if sim > 0.5:\n",
    "            print(\"Highly similar ✓\")\n",
    "        elif sim > 0.2:\n",
    "            print(\"Moderately similar\")\n",
    "        elif sim > -0.2:\n",
    "            print(\"Neutral/Unrelated\")\n",
    "        else:\n",
    "            print(\"Dissimilar ✓\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Embeddings capture semantic meaning through vector similarity.\")\n",
    "    print(\"Unlike one-hot encoding (always 0 similarity), embeddings\")\n",
    "    print(\"learn meaningful relationships between words!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors.\n",
    "    Returns value in [-1, 1], higher = more similar.\n",
    "    \"\"\"\n",
    "    dot = np.dot(vec_a, vec_b)\n",
    "    norm_a = np.linalg.norm(vec_a)\n",
    "    norm_b = np.linalg.norm(vec_b)\n",
    "    return dot / (norm_a * norm_b + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f0f89264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# TASK 3.4: Visualization\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_embeddings(embeddings, idx_to_word):\n",
    "    \"\"\"\n",
    "    Visualize embeddings in 2D using t-SNE or PCA.\n",
    "    \n",
    "    STEPS:\n",
    "    1. Reduce dimensions: sklearn.manifold.TSNE or sklearn.decomposition.PCA\n",
    "    2. Plot words as scatter points\n",
    "    3. Color by sentiment (if known)\n",
    "    4. Label each point with the word\n",
    "    \n",
    "    EXPECTED: Positive words cluster together, negative words cluster together\n",
    "    \n",
    "    SEE: demo_03_word2vec_visualization.py for complete visualization code\n",
    "    \n",
    "    Save to: embedding_visualization.png\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Task 3.4: Visualization\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # YOUR CODE:\n",
    "    # 1. Apply PCA or t-SNE to reduce to 2D\n",
    "    # 2. Plot with plt.scatter\n",
    "    # 3. Add word labels with plt.annotate\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# TASK 3.5: Embedding Dimension Experiment\n",
    "# ============================================================================\n",
    "\n",
    "def experiment_embedding_dims():\n",
    "    \"\"\"\n",
    "    Test how embedding dimension affects model performance.\n",
    "    \n",
    "    TEST DIMS: 8, 16, 32, 64, 128\n",
    "    \n",
    "    HYPOTHESIS: Higher dims = more capacity but more overfitting risk\n",
    "    \n",
    "    RECORD:\n",
    "    - Training accuracy for each dim\n",
    "    - Validation accuracy for each dim\n",
    "    - Training time\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Task 3.5: Embedding Dimension Experiment\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    import time\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Task 3.5: Embedding Dimension Experiment\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    texts = [\n",
    "        \"I love this movie it is amazing\",\n",
    "        \"Great film excellent acting wonderful\",\n",
    "        \"This movie is beautiful and amazing\",\n",
    "        \"Terrible movie waste of time awful\",\n",
    "        \"Bad film I hated it horrible\",\n",
    "        \"Disappointing and boring terrible waste\"\n",
    "    ]\n",
    "    labels = [1, 1, 1, 0, 0, 0]\n",
    "    \n",
    "    tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "        sequences, maxlen=max_length, padding='post'\n",
    "    )\n",
    "    \n",
    "    X = np.array(padded_sequences)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    dims = [8, 16, 32, 64, 128]\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nTesting embedding dimensions: {dims}\")\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    print(f\"Training samples: {len(X)}\")\n",
    "    print(f\"Using validation_split=0.33 for validation\\n\")\n",
    "    \n",
    "    print(f\"{'Dim':<8} {'Train Acc':<12} {'Val Acc':<12} {'Time (s)':<12} {'Params':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for embed_dim in dims:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model = keras.Sequential([\n",
    "            layers.Embedding(vocab_size, embed_dim, input_length=max_length),\n",
    "            layers.GlobalAveragePooling1D(),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            X, y,\n",
    "            epochs=100,\n",
    "            verbose=0,\n",
    "            validation_split=0.33\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        train_acc = history.history['accuracy'][-1]\n",
    "        val_acc = history.history['val_accuracy'][-1]\n",
    "        \n",
    "        embedding_params = vocab_size * embed_dim\n",
    "        dense_params = embed_dim * 1 + 1\n",
    "        params = embedding_params + dense_params\n",
    "        \n",
    "        results.append({\n",
    "            'dim': embed_dim,\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc,\n",
    "            'time': training_time,\n",
    "            'params': params\n",
    "        })\n",
    "        \n",
    "        print(f\"{embed_dim:<8} {train_acc:<12.4f} {val_acc:<12.4f} {training_time:<12.2f} {params:<12,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Analysis:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    best_val = max(results, key=lambda x: x['val_acc'])\n",
    "    fastest = min(results, key=lambda x: x['time'])\n",
    "    \n",
    "    print(f\"\\nBest validation accuracy: {best_val['val_acc']:.4f} (dim={best_val['dim']})\")\n",
    "    print(f\"Fastest training: {fastest['time']:.2f}s (dim={fastest['dim']})\")\n",
    "    \n",
    "    print(\"\\nObservations:\")\n",
    "    print(\"- Higher dimensions provide more capacity but may overfit\")\n",
    "    print(\"- Training time increases with dimension (more parameters)\")\n",
    "    print(\"- Optimal dimension balances capacity vs overfitting risk\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"NOTE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"sample size may be too small for accurate experimentation\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82661caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Exercise 03: Embedding Exploration\n",
      "============================================================\n",
      "============================================================\n",
      "Task 3.1: Embedding Layer Basics\n",
      "============================================================\n",
      "Vocabulary Size: 1000\n",
      "\n",
      "Embedding Dim   Parameters      Memory (MB)    \n",
      "---------------------------------------------\n",
      "16              16,000          0.0610         \n",
      "  Input shape: (1, 4)\n",
      "  Output shape: (1, 4, 16)\n",
      "\n",
      "32              32,000          0.1221         \n",
      "  Input shape: (1, 4)\n",
      "  Output shape: (1, 4, 32)\n",
      "\n",
      "64              64,000          0.2441         \n",
      "  Input shape: (1, 4)\n",
      "  Output shape: (1, 4, 64)\n",
      "\n",
      "128             128,000         0.4883         \n",
      "  Input shape: (1, 4)\n",
      "  Output shape: (1, 4, 128)\n",
      "\n",
      "256             256,000         0.9766         \n",
      "  Input shape: (1, 4)\n",
      "  Output shape: (1, 4, 256)\n",
      "\n",
      "\n",
      "============================================================\n",
      "Key Observations:\n",
      "============================================================\n",
      "1. Parameters = vocab_size × embedding_dim\n",
      "2. Memory = parameters × 4 bytes (float32)\n",
      "3. Output shape adds embedding_dim to input shape\n",
      "4. Higher dimensions = more capacity but more memory\n",
      "============================================================\n",
      "Task 3.2: Training Embeddings\n",
      "============================================================\n",
      "Vocabulary size: 25\n",
      "Word to index mapping: {'movie': 1, 'i': 2, 'this': 3, 'it': 4, 'is': 5, 'amazing': 6, 'film': 7, 'and': 8, 'terrible': 9, 'waste': 10, 'love': 11, 'great': 12, 'excellent': 13, 'acting': 14, 'wonderful': 15, 'beautiful': 16, 'of': 17, 'time': 18, 'awful': 19, 'bad': 20, 'hated': 21, 'horrible': 22, 'disappointing': 23, 'boring': 24}\n",
      "\n",
      "Sample sequences: [[2, 11, 3, 1, 4, 5, 6], [12, 7, 13, 14, 15]]\n",
      "\n",
      "Padded sequences shape: (6, 7)\n",
      "Labels shape: (6,)\n",
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle-anthonyhay/Documents/CODE/Revature-Training/Ai-Engineering/December/venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:100: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_58\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_58\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_168 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_58     │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_168 (\u001b[38;5;33mEmbedding\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_58     │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_58 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model...\n",
      "Final training loss: 0.5044\n",
      "Final training accuracy: 1.0000\n",
      "\n",
      "Embedding matrix shape: (25, 16)\n",
      "Embeddings for first 5 words:\n",
      "  'movie': [ 0.01841424 -0.01449239 -0.05531515  0.09051642 -0.09392176]...\n",
      "  'i': [-0.03516037  0.01319461 -0.03501838 -0.01548882 -0.03202377]...\n",
      "  'this': [ 0.10323714 -0.17077634 -0.17741868  0.16375753 -0.11663765]...\n",
      "  'it': [0.03138268 0.00988049 0.03247201 0.0311247  0.04561434]...\n",
      "  'is': [ 0.10104796 -0.17052098 -0.1702164   0.110618   -0.14405133]...\n",
      "\n",
      "============================================================\n",
      "Training Complete!\n",
      "============================================================\n",
      "============================================================\n",
      "Task 3.3: Semantic Similarity\n",
      "============================================================\n",
      "\n",
      "Cosine Similarity Analysis:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Pair: 'love' vs 'amazing'\n",
      "  Description: Both positive - should be similar\n",
      "  Cosine Similarity: 0.9472\n",
      "  Interpretation: Highly similar ✓\n",
      "\n",
      "Pair: 'terrible' vs 'awful'\n",
      "  Description: Both negative - should be similar\n",
      "  Cosine Similarity: 0.9402\n",
      "  Interpretation: Highly similar ✓\n",
      "'hate' not in vocabulary\n",
      "\n",
      "============================================================\n",
      "Summary:\n",
      "============================================================\n",
      "Embeddings capture semantic meaning through vector similarity.\n",
      "Unlike one-hot encoding (always 0 similarity), embeddings\n",
      "learn meaningful relationships between words!\n",
      "============================================================\n",
      "Task 3.4: Visualization\n",
      "============================================================\n",
      "============================================================\n",
      "Task 3.5: Embedding Dimension Experiment\n",
      "============================================================\n",
      "============================================================\n",
      "Task 3.5: Embedding Dimension Experiment\n",
      "============================================================\n",
      "\n",
      "Testing embedding dimensions: [8, 16, 32, 64, 128]\n",
      "Vocabulary size: 25\n",
      "Training samples: 6\n",
      "Using validation_split=0.33 for validation\n",
      "\n",
      "Dim      Train Acc    Val Acc      Time (s)     Params      \n",
      "------------------------------------------------------------\n",
      "8        1.0000       0.0000       2.09         209         \n",
      "16       1.0000       0.0000       2.06         417         \n",
      "32       1.0000       0.0000       2.04         833         \n",
      "64       1.0000       0.0000       2.03         1,665       \n",
      "128      1.0000       0.0000       1.90         3,329       \n",
      "\n",
      "============================================================\n",
      "Analysis:\n",
      "============================================================\n",
      "\n",
      "Best validation accuracy: 0.0000 (dim=8)\n",
      "Fastest training: 1.90s (dim=128)\n",
      "\n",
      "Observations:\n",
      "- Higher dimensions provide more capacity but may overfit\n",
      "- Training time increases with dimension (more parameters)\n",
      "- Optimal dimension balances capacity vs overfitting risk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Exercise 03: Embedding Exploration\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Uncomment as you complete:\n",
    "    explore_embedding_layer()\n",
    "    embeddings, word_to_idx, idx_to_word = train_sentiment_classifier()\n",
    "    analyze_semantic_similarity(embeddings, word_to_idx)\n",
    "    visualize_embeddings(embeddings, idx_to_word)\n",
    "    experiment_embedding_dims() # sample size may be too small for accurate experimentation\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
