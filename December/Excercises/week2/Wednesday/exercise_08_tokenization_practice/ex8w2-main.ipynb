{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c04717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "633fa269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Basic word tokenization using whitespace and punctuation.\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert to lowercase\n",
    "    2. Split on whitespace and punctuation\n",
    "    3. Remove empty tokens\n",
    "    \n",
    "    Example:\n",
    "        \"Hello, World!\" -> [\"hello\", \"world\"]\n",
    "    \n",
    "    Args:\n",
    "        text: Input string\n",
    "    \n",
    "    Returns:\n",
    "        List of tokens\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = re.split(r'\\W+', text)\n",
    "    tokens = [token for token in tokens if token]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7fcb8c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_punctuation(text):\n",
    "    \"\"\"\n",
    "    Tokenize while preserving punctuation as separate tokens.\n",
    "    \n",
    "    Example:\n",
    "        \"Hello, World!\" -> [\"hello\", \",\", \"world\", \"!\"]\n",
    "    \n",
    "    Hint: Use re.findall(\"\\\\w+|[^\\\\w\\\\s]\", text.lower())\n",
    "    \"\"\"\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c1cec2",
   "metadata": {},
   "source": [
    "## Task 1.2: Test Your Tokenizer\n",
    "\n",
    "Test with these edge cases:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f009327a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Hello, World!\n",
      "Simple: ['hello', 'world']\n",
      "With punct: ['hello', ',', 'world', '!']\n",
      "\n",
      "Input: I'm learning NLP.\n",
      "Simple: ['i', 'm', 'learning', 'nlp']\n",
      "With punct: ['i', \"'\", 'm', 'learning', 'nlp', '.']\n",
      "\n",
      "Input: Email: test@example.com\n",
      "Simple: ['email', 'test', 'example', 'com']\n",
      "With punct: ['email', ':', 'test', '@', 'example', '.', 'com']\n",
      "\n",
      "Input: The price is $19.99!\n",
      "Simple: ['the', 'price', 'is', '19', '99']\n",
      "With punct: ['the', 'price', 'is', '$', '19', '.', '99', '!']\n",
      "\n",
      "Input:    Multiple   spaces   here   \n",
      "Simple: ['multiple', 'spaces', 'here']\n",
      "With punct: ['multiple', 'spaces', 'here']\n",
      "\n",
      "Input: UPPERCASE and lowercase\n",
      "Simple: ['uppercase', 'and', 'lowercase']\n",
      "With punct: ['uppercase', 'and', 'lowercase']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    \"Hello, World!\",                    # Basic punctuation\n",
    "    \"I'm learning NLP.\",                # Contractions\n",
    "    \"Email: test@example.com\",          # Special characters\n",
    "    \"The price is $19.99!\",             # Numbers and symbols\n",
    "    \"   Multiple   spaces   here   \",   # Extra whitespace\n",
    "    \"UPPERCASE and lowercase\",          # Case handling\n",
    "]\n",
    "\n",
    "for text in test_cases:\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Simple: {simple_tokenize(text)}\")\n",
    "    print(f\"With punct: {tokenize_with_punctuation(text)}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f684dc",
   "metadata": {},
   "source": [
    "## Part 2: Vocabulary Building (15 min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0129f7",
   "metadata": {},
   "source": [
    "### Task 2.1: Build Vocabulary from Corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61e58a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Vocabulary manager for tokenized text.\n",
    "    \n",
    "    Special tokens:\n",
    "    - <PAD>: Padding token (index 0)\n",
    "    - <UNK>: Unknown token (index 1)\n",
    "    - <BOS>: Beginning of sequence (index 2)\n",
    "    - <EOS>: End of sequence (index 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_freq=1):\n",
    "        \"\"\"\n",
    "        Initialize vocabulary with special tokens.\n",
    "        \n",
    "        Args:\n",
    "            min_freq: Minimum frequency for a token to be included\n",
    "        \"\"\"\n",
    "        self.min_freq = min_freq\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1, \"<BOS>\": 2, \"<EOS>\": 3}\n",
    "        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\", 2: \"<BOS>\", 3: \"<EOS>\"}\n",
    "        self.word_freq = Counter()\n",
    "    \n",
    "    def build_vocab(self, texts, tokenizer_fn):\n",
    "        \"\"\"\n",
    "        Build vocabulary from a list of texts.\n",
    "        \n",
    "        Steps:\n",
    "        1. Tokenize each text\n",
    "        2. Count word frequencies\n",
    "        3. Add words meeting min_freq threshold\n",
    "        \n",
    "        Args:\n",
    "            texts: List of strings\n",
    "            tokenizer_fn: Function to tokenize each text\n",
    "        \"\"\"\n",
    "        for text in texts:\n",
    "            tokens = tokenizer_fn(text)\n",
    "            self.word_freq.update(tokens)\n",
    "        \n",
    "        for word, freq in self.word_freq.items():\n",
    "            if freq >= self.min_freq and word not in self.word2idx:\n",
    "                idx = len(self.word2idx)\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "    \n",
    "    def encode(self, tokens):\n",
    "        \"\"\"\n",
    "        Convert list of tokens to list of indices.\n",
    "        \n",
    "        Unknown tokens map to <UNK> (index 1).\n",
    "        \"\"\"\n",
    "        return [self.word2idx.get(token, 1) for token in tokens]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert list of indices back to tokens.\"\"\"\n",
    "        return [self.idx2word.get(idx, \"<UNK>\") for idx in indices]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757839dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d22193e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10\n",
      "Words: ['<PAD>', '<UNK>', '<BOS>', '<EOS>', 'the', 'cat', 'on', 'mat', 'dog', 'a']\n",
      "\n",
      "Tokens: ['the', 'cat', 'is', 'happy']\n",
      "Encoded: [4, 5, 1, 1]\n",
      "Decoded: ['the', 'cat', '<UNK>', '<UNK>']\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog ran in the park.\",\n",
    "    \"A cat and a dog are friends.\",\n",
    "    \"The mat is on the floor.\"\n",
    "]\n",
    "\n",
    "vocab = Vocabulary(min_freq=2)\n",
    "vocab.build_vocab(corpus, simple_tokenize)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Words: {list(vocab.word2idx.keys())}\")\n",
    "\n",
    "test_text = \"The cat is happy\"\n",
    "tokens = simple_tokenize(test_text)\n",
    "encoded = vocab.encode(tokens)\n",
    "decoded = vocab.decode(encoded)\n",
    "\n",
    "print(f\"\\nTokens: {tokens}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bb65e8",
   "metadata": {},
   "source": [
    "## Part 3: Complete Tokenization Pipeline (15 min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0752b8f",
   "metadata": {},
   "source": [
    "### Task 3.1: Build TextPreprocessor Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca5b3f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Complete text preprocessing pipeline.\n",
    "    \n",
    "    Usage:\n",
    "        preprocessor = TextPreprocessor()\n",
    "        preprocessor.fit(train_texts)\n",
    "        train_sequences = preprocessor.transform(train_texts)\n",
    "        test_sequences = preprocessor.transform(test_texts)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_vocab_size=10000, min_freq=2, max_seq_length=100):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_freq = min_freq\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.vocab = Vocabulary(min_freq=min_freq)\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        \"\"\"Build vocabulary from training texts.\"\"\"\n",
    "        temp_vocab = Vocabulary(min_freq=self.min_freq)\n",
    "        temp_vocab.build_vocab(texts, simple_tokenize)\n",
    "        \n",
    "        if len(temp_vocab.word_freq) > 0:\n",
    "            sorted_words = sorted(temp_vocab.word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "            limited_words = sorted_words[:self.max_vocab_size - 4]\n",
    "            \n",
    "            self.vocab.word_freq = Counter(dict(sorted_words))\n",
    "            for word, freq in limited_words:\n",
    "                if word not in self.vocab.word2idx:\n",
    "                    idx = len(self.vocab.word2idx)\n",
    "                    self.vocab.word2idx[word] = idx\n",
    "                    self.vocab.idx2word[idx] = word\n",
    "    \n",
    "    def transform(self, texts):\n",
    "        \"\"\"\n",
    "        Transform texts to padded sequences.\n",
    "        \n",
    "        Steps:\n",
    "        1. Tokenize each text\n",
    "        2. Encode tokens to indices\n",
    "        3. Pad/truncate to max_seq_length\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            tokens = simple_tokenize(text)\n",
    "            encoded = self.vocab.encode(tokens)\n",
    "            \n",
    "            if len(encoded) > self.max_seq_length:\n",
    "                encoded = encoded[:self.max_seq_length]\n",
    "            else:\n",
    "                encoded = encoded + [0] * (self.max_seq_length - len(encoded))\n",
    "            \n",
    "            sequences.append(encoded)\n",
    "        \n",
    "        return np.array(sequences)\n",
    "    \n",
    "    def fit_transform(self, texts):\n",
    "        \"\"\"Fit and transform in one step.\"\"\"\n",
    "        self.fit(texts)\n",
    "        return self.transform(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f36e5",
   "metadata": {},
   "source": [
    "### Task 3.2: Test on Real Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cccba6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 36\n",
      "Sequence shape: (5, 20)\n",
      "\n",
      "Sample sequence: [ 5  6  9 10 11 12 13  4 14  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Decoded: ['this', 'movie', 'was', 'absolutely', 'fantastic', 'great', 'acting', 'and', 'plot', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "reviews = [\n",
    "    \"This movie was absolutely fantastic! Great acting and plot.\",\n",
    "    \"Terrible film. Waste of time and money.\",\n",
    "    \"An average movie. Nothing special but watchable.\",\n",
    "    \"One of the best movies I've ever seen!\",\n",
    "    \"Boring and predictable. Skip this one.\"\n",
    "]\n",
    "\n",
    "preprocessor = TextPreprocessor(max_seq_length=20)\n",
    "sequences = preprocessor.fit_transform(reviews)\n",
    "\n",
    "print(f\"Vocabulary size: {len(preprocessor.vocab)}\")\n",
    "print(f\"Sequence shape: {sequences.shape}\")\n",
    "print(f\"\\nSample sequence: {sequences[0]}\")\n",
    "print(f\"Decoded: {preprocessor.vocab.decode(sequences[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695145e7",
   "metadata": {},
   "source": [
    "## Reflection Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa1cb2",
   "metadata": {},
   "source": [
    "### 1. How does min_freq affect vocabulary size?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d778d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_freq=1: Vocabulary size = 23\n",
      "  Words: ['a', 'and', 'are', 'cat', 'cats', 'dog', 'dogs', 'floor', 'friends', 'i', 'in', 'is', 'love', 'mat', 'on', 'park', 'ran', 'sat', 'the']\n",
      "\n",
      "min_freq=2: Vocabulary size = 11\n",
      "  Words: ['a', 'and', 'cat', 'dog', 'mat', 'on', 'the']\n",
      "\n",
      "min_freq=5: Vocabulary size = 5\n",
      "  Words: ['the']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_corpus = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog ran in the park.\",\n",
    "    \"A cat and a dog are friends.\",\n",
    "    \"The mat is on the floor.\",\n",
    "    \"I love cats and dogs.\"\n",
    "]\n",
    "\n",
    "for min_freq in [1, 2, 5]:\n",
    "    vocab = Vocabulary(min_freq=min_freq)\n",
    "    vocab.build_vocab(test_corpus, simple_tokenize)\n",
    "    print(f\"min_freq={min_freq}: Vocabulary size = {len(vocab)}\")\n",
    "    print(f\"  Words: {sorted([w for w in vocab.word2idx.keys() if w not in ['<PAD>', '<UNK>', '<BOS>', '<EOS>']])}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b3fcc2",
   "metadata": {},
   "source": [
    "**Answer:** Higher `min_freq` filters out rare words, reducing vocabulary size. This helps:\n",
    "- Reduce memory usage and computation\n",
    "- Focus on common, meaningful words\n",
    "- Teh trade of is that may lose important rare words (e.g., domain-specific terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c2df5",
   "metadata": {},
   "source": [
    "### 2. What happens when a test text contains words not in the vocabulary?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e87d1884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text: 'The elephant is happy'\n",
      "Tokens: ['the', 'elephant', 'is', 'happy']\n",
      "Encoded: [4, 1, 1, 1]\n",
      "Decoded: ['the', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "Note: 'elephant' and 'happy' → <UNK> (index 1) because they're not in training vocabulary\n"
     ]
    }
   ],
   "source": [
    "train_corpus = [\"The cat sat on the mat.\", \"The dog ran in the park.\"]\n",
    "test_text = \"The elephant is happy\"\n",
    "\n",
    "vocab = Vocabulary(min_freq=1)\n",
    "vocab.build_vocab(train_corpus, simple_tokenize)\n",
    "\n",
    "tokens = simple_tokenize(test_text)\n",
    "encoded = vocab.encode(tokens)\n",
    "decoded = vocab.decode(encoded)\n",
    "\n",
    "print(f\"Test text: '{test_text}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "print(f\"\\nNote: 'elephant' and 'happy' → <UNK> (index 1) because they're not in training vocabulary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f216e83",
   "metadata": {},
   "source": [
    "**Answer:** Unknown words map to `<UNK>`. Impact on model performance may lsoe some semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef7050c",
   "metadata": {},
   "source": [
    "### 3. How do you choose max_seq_length?\n",
    "\n",
    "**Answer:** Balance between information retention and efficiency with computation:\n",
    "- If its too short it will truncate important information, as well as losing context\n",
    "- Wastes memory due to padding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bb0ac0",
   "metadata": {},
   "source": [
    "### 4. Why do we fit on training data only, then transform both train and test?\n",
    "\n",
    "**Answer:** Keeps the integrity fo the evaluation by making sure the model is only tested on data its never seen before.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
