{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f50a585",
   "metadata": {},
   "source": [
    "# Exercise 09: One-Hot vs. Dense Encoding\n",
    "\n",
    "## Part 1: Implement One-Hot Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f16b44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4998f7cc",
   "metadata": {},
   "source": [
    "### Task 1.1: Create One-Hot Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e1e3d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoder:\n",
    "    \"\"\"\n",
    "    One-hot encoder for text vocabulary.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    def fit(self, words):\n",
    "        \"\"\"\n",
    "        Build vocabulary from list of words.\n",
    "        \n",
    "        Args:\n",
    "            words: List of unique words\n",
    "        \"\"\"\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(words)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        self.vocab_size = len(words)\n",
    "    \n",
    "    def encode_word(self, word):\n",
    "        \"\"\"\n",
    "        Encode single word as one-hot vector.\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of shape (vocab_size,)\n",
    "        \"\"\"\n",
    "        vec = np.zeros(self.vocab_size)\n",
    "        if word in self.word2idx:\n",
    "            vec[self.word2idx[word]] = 1\n",
    "        return vec\n",
    "    \n",
    "    def encode_sequence(self, words):\n",
    "        \"\"\"\n",
    "        Encode list of words as matrix.\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of shape (len(words), vocab_size)\n",
    "        \"\"\"\n",
    "        return np.array([self.encode_word(word) for word in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96c621f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['cat', 'dog', 'bird', 'fish', 'horse']\n",
      "Vocabulary size: 5\n",
      "\n",
      "Word to Index mapping:\n",
      "  'cat' -> 0\n",
      "  'dog' -> 1\n",
      "  'bird' -> 2\n",
      "  'fish' -> 3\n",
      "  'horse' -> 4\n",
      "\n",
      "One-hot vectors for each word:\n",
      "  'cat': [1 0 0 0 0]\n",
      "  'dog': [0 1 0 0 0]\n",
      "  'bird': [0 0 1 0 0]\n",
      "  'fish': [0 0 0 1 0]\n",
      "  'horse': [0 0 0 0 1]\n",
      "\n",
      "Sequence: ['cat', 'dog', 'bird']\n",
      "Encoded shape: (3, 5)\n",
      "Encoded matrix:\n",
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Test the OneHotEncoder\n",
    "vocabulary = [\"cat\", \"dog\", \"bird\", \"fish\", \"horse\"]\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(vocabulary)\n",
    "\n",
    "print(f\"Vocabulary: {vocabulary}\")\n",
    "print(f\"Vocabulary size: {encoder.vocab_size}\")\n",
    "print(f\"\\nWord to Index mapping:\")\n",
    "for word, idx in encoder.word2idx.items():\n",
    "    print(f\"  '{word}' -> {idx}\")\n",
    "\n",
    "print(\"\\nOne-hot vectors for each word:\")\n",
    "for word in vocabulary:\n",
    "    vec = encoder.encode_word(word)\n",
    "    print(f\"  '{word}': {vec.astype(int)}\")\n",
    "\n",
    "# Test encode_sequence\n",
    "sentence = [\"cat\", \"dog\", \"bird\"]\n",
    "encoded = encoder.encode_sequence(sentence)\n",
    "print(f\"\\nSequence: {sentence}\")\n",
    "print(f\"Encoded shape: {encoded.shape}\")\n",
    "print(f\"Encoded matrix:\\n{encoded.astype(int)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51849ae",
   "metadata": {},
   "source": [
    "### Task 1.2: Measure Memory Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "978a42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_memory(vocab_size, seq_length, dtype=np.float32):\n",
    "    \"\"\"\n",
    "    Calculate memory usage for one-hot encoding.\n",
    "    \n",
    "    Returns:\n",
    "        Memory in bytes and human-readable format\n",
    "    \"\"\"\n",
    "    bytes_per_element = np.dtype(dtype).itemsize\n",
    "    total_bytes = vocab_size * seq_length * bytes_per_element\n",
    "    \n",
    "    if total_bytes < 1024:\n",
    "        return f\"{total_bytes} bytes\"\n",
    "    elif total_bytes < 1024 * 1024:\n",
    "        return f\"{total_bytes / 1024:.2f} KB\"\n",
    "    elif total_bytes < 1024 * 1024 * 1024:\n",
    "        return f\"{total_bytes / (1024 * 1024):.2f} MB\"\n",
    "    else:\n",
    "        return f\"{total_bytes / (1024 * 1024 * 1024):.2f} GB\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7884d884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage for one-hot encoding:\n",
      "--------------------------------------------------\n",
      "Vocab 1,000: 390.62 KB\n",
      "Vocab 10,000: 3.81 MB\n",
      "Vocab 50,000: 19.07 MB\n",
      "Vocab 100,000: 38.15 MB\n"
     ]
    }
   ],
   "source": [
    "# Test with realistic NLP vocabulary sizes\n",
    "vocab_sizes = [1000, 10000, 50000, 100000]\n",
    "seq_length = 100\n",
    "\n",
    "print(\"Memory usage for one-hot encoding:\")\n",
    "print(\"-\" * 50)\n",
    "for v in vocab_sizes:\n",
    "    mem = measure_memory(v, seq_length)\n",
    "    print(f\"Vocab {v:,}: {mem}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1963a",
   "metadata": {},
   "source": [
    "## Part 2: The Similarity Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f755b4",
   "metadata": {},
   "source": [
    "### Task 2.1: Compute Cosine Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e461f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec_a, vec_b):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors.\n",
    "    \n",
    "    Formula: cos_sim = (A dot B) / (||A|| * ||B||)\n",
    "    \n",
    "    Returns:\n",
    "        Similarity score between -1 and 1\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec_a, vec_b)\n",
    "    norm_a = np.linalg.norm(vec_a)\n",
    "    norm_b = np.linalg.norm(vec_b)\n",
    "    \n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0\n",
    "    \n",
    "    return dot_product / (norm_a * norm_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd4d98",
   "metadata": {},
   "source": [
    "### Task 2.2: Demonstrate the Problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7341bf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Similarities:\n",
      "----------------------------------------\n",
      "cat vs dog: 0.0000\n",
      "car vs truck: 0.0000\n",
      "happy vs sad: 0.0000\n",
      "cat vs car: 0.0000\n",
      "happy vs truck: 0.0000\n",
      "\n",
      "Problem: All similarities are 0\n",
      "One-hot cannot distinguish semantic relationships between differemt words.\n"
     ]
    }
   ],
   "source": [
    "# Create encoder with semantic word groups\n",
    "words = [\n",
    "    \"cat\", \"dog\", \"bird\",           # Animals\n",
    "    \"car\", \"truck\", \"motorcycle\",   # Vehicles\n",
    "    \"happy\", \"sad\", \"angry\",        # Emotions\n",
    "    \"run\", \"walk\", \"jump\"           # Actions\n",
    "]\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(words)\n",
    "\n",
    "# Compute similarity between all pairs\n",
    "print(\"One-Hot Similarities:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Compare semantically similar words\n",
    "pairs_similar = [(\"cat\", \"dog\"), (\"car\", \"truck\"), (\"happy\", \"sad\")]\n",
    "for w1, w2 in pairs_similar:\n",
    "    v1 = encoder.encode_word(w1)\n",
    "    v2 = encoder.encode_word(w2)\n",
    "    sim = cosine_similarity(v1, v2)\n",
    "    print(f\"{w1} vs {w2}: {sim:.4f}\")  # Should all be 0!\n",
    "\n",
    "# Compare semantically different words\n",
    "pairs_different = [(\"cat\", \"car\"), (\"happy\", \"truck\")]\n",
    "for w1, w2 in pairs_different:\n",
    "    v1 = encoder.encode_word(w1)\n",
    "    v2 = encoder.encode_word(w2)\n",
    "    sim = cosine_similarity(v1, v2)\n",
    "    print(f\"{w1} vs {w2}: {sim:.4f}\")  # Also 0!\n",
    "\n",
    "print(\"\\nProblem: All similarities are 0\")\n",
    "print(\"One-hot cannot distinguish semantic relationships between differemt words.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a62d86",
   "metadata": {},
   "source": [
    "## Part 3: Compare with Dense Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c55d0",
   "metadata": {},
   "source": [
    "### Task 3.1: Simulate Dense Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36f82350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_semantic_embeddings(words, embedding_dim=8):\n",
    "    \"\"\"\n",
    "    Create dense embeddings with semantic structure.\n",
    "    \n",
    "    Words in similar categories will have similar embeddings.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Define semantic categories\n",
    "    categories = {\n",
    "        \"animal\": [\"cat\", \"dog\", \"bird\"],\n",
    "        \"vehicle\": [\"car\", \"truck\", \"motorcycle\"],\n",
    "        \"emotion\": [\"happy\", \"sad\", \"angry\"],\n",
    "        \"action\": [\"run\", \"walk\", \"jump\"]\n",
    "    }\n",
    "    \n",
    "    # Create category base vectors\n",
    "    category_vectors = {\n",
    "        cat: np.random.randn(embedding_dim) for cat in categories\n",
    "    }\n",
    "    \n",
    "    embeddings = {}\n",
    "    for category, word_list in categories.items():\n",
    "        base = category_vectors[category]\n",
    "        for word in word_list:\n",
    "            # Add small random noise to base vector\n",
    "            noise = np.random.randn(embedding_dim) * 0.1\n",
    "            embeddings[word] = base + noise\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275265c0",
   "metadata": {},
   "source": [
    "### Task 3.2: Compare Similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "618b534a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dense Embedding Similarities:\n",
      "----------------------------------------\n",
      "cat vs dog: 0.9931\n",
      "car vs truck: 0.9906\n",
      "happy vs sad: 0.9946\n",
      "cat vs car: -0.5199\n",
      "happy vs truck: 0.3487\n"
     ]
    }
   ],
   "source": [
    "embeddings = create_semantic_embeddings(words)\n",
    "\n",
    "print(\"\\nDense Embedding Similarities:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Same pairs as before\n",
    "for w1, w2 in pairs_similar:\n",
    "    sim = cosine_similarity(embeddings[w1], embeddings[w2])\n",
    "    print(f\"{w1} vs {w2}: {sim:.4f}\")  # Should be HIGH (similar categories)\n",
    "\n",
    "for w1, w2 in pairs_different:\n",
    "    sim = cosine_similarity(embeddings[w1], embeddings[w2])\n",
    "    print(f\"{w1} vs {w2}: {sim:.4f}\")  # Should be LOW (different categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cb245a",
   "metadata": {},
   "source": [
    "### Task 3.3: Memory Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c644952c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Comparison (vocab_size=50,000, seq_length=100, embedding_dim=100):\n",
      "------------------------------------------------------------\n",
      "One-hot encoding: 19.07 MB\n",
      "Dense embeddings: 39.06 KB\n",
      "Ratio (one-hot / dense): 500.0x\n",
      "\n",
      "Dense embeddings use 500.0x less memory!\n"
     ]
    }
   ],
   "source": [
    "def compare_memory(vocab_size, seq_length, embedding_dim=100):\n",
    "    \"\"\"Compare memory usage between one-hot and dense embeddings.\"\"\"\n",
    "    \n",
    "    one_hot_bytes = vocab_size * seq_length * 4  # float32\n",
    "    dense_bytes = embedding_dim * seq_length * 4  # float32\n",
    "    \n",
    "    ratio = one_hot_bytes / dense_bytes\n",
    "    \n",
    "    print(f\"Memory Comparison (vocab_size={vocab_size:,}, seq_length={seq_length}, embedding_dim={embedding_dim}):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"One-hot encoding: {measure_memory(vocab_size, seq_length)}\")\n",
    "    print(f\"Dense embeddings: {measure_memory(embedding_dim, seq_length)}\")\n",
    "    print(f\"Ratio (one-hot / dense): {ratio:.1f}x\")\n",
    "    print(f\"\\nDense embeddings use {ratio:.1f}x less memory!\")\n",
    "\n",
    "compare_memory(50000, 100, embedding_dim=100)\n",
    "# Expected: Dense uses ~500x less memory!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad0d9f2",
   "metadata": {},
   "source": [
    "## Summary Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c782d7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Table:\n",
      "======================================================================\n",
      "Metric                                   One-Hot              Dense (dim=100)     \n",
      "----------------------------------------------------------------------\n",
      "Memory per word (50K vocab)              195.31 KB            400 bytes           \n",
      "Memory per sequence (100 words)          19.07 MB             39.06 KB            \n",
      "Similarity: cat-dog                      0.0000               0.9931              \n",
      "Similarity: cat-car                      0.0000               -0.5199             \n",
      "Captures semantics?                      No                   Yes                 \n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate values for summary table\n",
    "vocab_size = 50000\n",
    "seq_length = 100\n",
    "embedding_dim = 100\n",
    "\n",
    "# Memory per word (one-hot)\n",
    "one_hot_per_word_bytes = vocab_size * 4  # float32\n",
    "one_hot_per_word = measure_memory(vocab_size, 1)\n",
    "\n",
    "# Memory per word (dense)\n",
    "dense_per_word_bytes = embedding_dim * 4  # float32\n",
    "dense_per_word = measure_memory(embedding_dim, 1)\n",
    "\n",
    "# Memory per sequence (one-hot)\n",
    "one_hot_per_seq = measure_memory(vocab_size, seq_length)\n",
    "\n",
    "# Memory per sequence (dense)\n",
    "dense_per_seq = measure_memory(embedding_dim, seq_length)\n",
    "\n",
    "# Similarities (from previous cells)\n",
    "# One-hot: cat-dog = 0.0000, cat-car = 0.0000\n",
    "# Dense: cat-dog = 0.9931, cat-car = -0.5199\n",
    "\n",
    "print(\"Summary Table:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<40} {'One-Hot':<20} {'Dense (dim=100)':<20}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Memory per word (50K vocab)':<40} {one_hot_per_word:<20} {dense_per_word:<20}\")\n",
    "print(f\"{'Memory per sequence (100 words)':<40} {one_hot_per_seq:<20} {dense_per_seq:<20}\")\n",
    "print(f\"{'Similarity: cat-dog':<40} {'0.0000':<20} {'0.9931':<20}\")\n",
    "print(f\"{'Similarity: cat-car':<40} {'0.0000':<20} {'-0.5199':<20}\")\n",
    "print(f\"{'Captures semantics?':<40} {'No':<20} {'Yes':<20}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3a7505",
   "metadata": {},
   "source": [
    "## Reflection Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b86c54",
   "metadata": {},
   "source": [
    "### 1. What is the sparsity of one-hot vectors? (What percentage of values are zero?) How does this waste computation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "97dc16e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of One-Hot Vectors:\n",
      "--------------------------------------------------\n",
      "Vocab size     100: 99.0000% zeros (1/100 non-zero)\n",
      "Vocab size   1,000: 99.9000% zeros (1/1000 non-zero)\n",
      "Vocab size  10,000: 99.9900% zeros (1/10000 non-zero)\n",
      "Vocab size  50,000: 99.9980% zeros (1/50000 non-zero)\n",
      "Vocab size 100,000: 99.9990% zeros (1/100000 non-zero)\n",
      "\n",
      "Answer:\n",
      "One-hot vectors are extremely sparse:\n",
      "- For a vocabulary of 50,000 words, 99.998% of values are zeros\n",
      "- Only 1 out of 50,000 positions contains a 1\n",
      "\n",
      "This wastes computation because:\n",
      "1. Most operations multiply by zero (no effect)\n",
      "2. Memory is allocated for all zeros but never used\n",
      "3. Matrix operations become inefficient with sparse data\n",
      "4. Cache misses increase due to large memory footprint\n"
     ]
    }
   ],
   "source": [
    "# Calculate sparsity for different vocabulary sizes\n",
    "vocab_sizes = [100, 1000, 10000, 50000, 100000]\n",
    "\n",
    "print(\"Sparsity of One-Hot Vectors:\")\n",
    "print(\"-\" * 50)\n",
    "for vocab_size in vocab_sizes:\n",
    "    nonzero = 1  # Only one position is 1\n",
    "    total = vocab_size\n",
    "    sparsity = (total - nonzero) / total * 100\n",
    "    print(f\"Vocab size {vocab_size:>7,}: {sparsity:.4f}% zeros ({nonzero}/{total} non-zero)\")\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(\"One-hot vectors are extremely sparse:\")\n",
    "print(\"- For a vocabulary of 50,000 words, 99.998% of values are zeros\")\n",
    "print(\"- Only 1 out of 50,000 positions contains a 1\")\n",
    "print(\"\\nThis wastes computation because:\")\n",
    "print(\"1. Most operations multiply by zero (no effect)\")\n",
    "print(\"2. Memory is allocated for all zeros but never used\")\n",
    "print(\"3. Matrix operations become inefficient with sparse data\")\n",
    "print(\"4. Cache misses increase due to large memory footprint\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec8ca9",
   "metadata": {},
   "source": [
    "### 2. Why do one-hot vectors have zero similarity? Think about what the dot product of two one-hot vectors equals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01021249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: cat and dog one-hot vectors\n",
      "cat vector: [1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "dog vector: [0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Dot product (cat · dog): 0.0\n",
      "||cat|| = 1.0000\n",
      "||dog|| = 1.0000\n",
      "\n",
      "Cosine similarity: 0.0000\n",
      "\n",
      "Answer:\n",
      "One-hot vectors have zero similarity because:\n",
      "1. Each one-hot vector has exactly one '1' and all other positions are '0'\n",
      "2. Different words have their '1' at different positions\n",
      "3. The dot product of two different one-hot vectors is always 0\n",
      "   (since they never have '1' at the same position)\n",
      "4. Cosine similarity = (A · B) / (||A|| * ||B||)\n",
      "   Since A · B = 0, cosine similarity = 0\n",
      "5. One-hot vectors are orthogonal (perpendicular) to each other\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate why one-hot vectors have zero similarity\n",
    "cat_vec = encoder.encode_word(\"cat\")\n",
    "dog_vec = encoder.encode_word(\"dog\")\n",
    "\n",
    "print(\"Example: cat and dog one-hot vectors\")\n",
    "print(f\"cat vector: {cat_vec.astype(int)}\")\n",
    "print(f\"dog vector: {dog_vec.astype(int)}\")\n",
    "print()\n",
    "\n",
    "# Dot product\n",
    "dot_product = np.dot(cat_vec, dog_vec)\n",
    "print(f\"Dot product (cat · dog): {dot_product}\")\n",
    "\n",
    "# Norms\n",
    "norm_cat = np.linalg.norm(cat_vec)\n",
    "norm_dog = np.linalg.norm(dog_vec)\n",
    "print(f\"||cat|| = {norm_cat:.4f}\")\n",
    "print(f\"||dog|| = {norm_dog:.4f}\")\n",
    "print()\n",
    "\n",
    "# Cosine similarity\n",
    "cos_sim = cosine_similarity(cat_vec, dog_vec)\n",
    "print(f\"Cosine similarity: {cos_sim:.4f}\")\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(\"One-hot vectors have zero similarity because:\")\n",
    "print(\"1. Each one-hot vector has exactly one '1' and all other positions are '0'\")\n",
    "print(\"2. Different words have their '1' at different positions\")\n",
    "print(\"3. The dot product of two different one-hot vectors is always 0\")\n",
    "print(\"   (since they never have '1' at the same position)\")\n",
    "print(\"4. Cosine similarity = (A · B) / (||A|| * ||B||)\")\n",
    "print(\"   Since A · B = 0, cosine similarity = 0\")\n",
    "print(\"5. One-hot vectors are orthogonal (perpendicular) to each other\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d95c2",
   "metadata": {},
   "source": [
    "### 3. How do dense embeddings capture \"cat is similar to dog\"? What property of the vectors enables this?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f54faaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Embeddings (first 5 dimensions):\n",
      "cat:  [ 0.49536443 -0.24403539  0.72994303  1.40094549 -0.21326702]\n",
      "dog:  [ 0.57056081 -0.12112747  0.63612371  1.49291949 -0.38200557]\n",
      "car:  [-0.55339614  0.51163881 -0.43029135 -0.36817524  0.19404485]\n",
      "\n",
      "Similarity cat-dog: 0.9931\n",
      "Similarity cat-car: -0.5199\n",
      "\n",
      "Key properties:\n",
      "1. Both cat and dog embeddings have non-zero values in ALL dimensions\n",
      "2. They share similar values because they come from the same category base vector\n",
      "3. The dot product is non-zero (unlike one-hot)\n",
      "4. Cosine similarity captures the angle between vectors in high-dimensional space\n",
      "\n",
      "Answer:\n",
      "Dense embeddings capture semantic similarity through:\n",
      "1. Shared dimensions: Both vectors have values in the same dimensions\n",
      "2. Similar magnitudes: Words in the same category have similar vector values\n",
      "3. Non-zero dot product: Unlike one-hot, dense vectors can have non-zero dot products\n",
      "4. Geometric proximity: Similar words are close in the embedding space\n",
      "5. Learned representations: Embeddings can be trained to capture semantic relationships\n",
      "6. Continuous values: All dimensions contribute to similarity, not just one position\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate dense embeddings\n",
    "cat_emb = embeddings[\"cat\"]\n",
    "dog_emb = embeddings[\"dog\"]\n",
    "car_emb = embeddings[\"car\"]\n",
    "\n",
    "print(\"Dense Embeddings (first 5 dimensions):\")\n",
    "print(f\"cat:  {cat_emb[:5]}\")\n",
    "print(f\"dog:  {dog_emb[:5]}\")\n",
    "print(f\"car:  {car_emb[:5]}\")\n",
    "print()\n",
    "\n",
    "# Show they share similar values\n",
    "cat_dog_sim = cosine_similarity(cat_emb, dog_emb)\n",
    "cat_car_sim = cosine_similarity(cat_emb, car_emb)\n",
    "\n",
    "print(f\"Similarity cat-dog: {cat_dog_sim:.4f}\")\n",
    "print(f\"Similarity cat-car: {cat_car_sim:.4f}\")\n",
    "print()\n",
    "\n",
    "# Show they're in the same semantic space\n",
    "print(\"Key properties:\")\n",
    "print(\"1. Both cat and dog embeddings have non-zero values in ALL dimensions\")\n",
    "print(\"2. They share similar values because they come from the same category base vector\")\n",
    "print(\"3. The dot product is non-zero (unlike one-hot)\")\n",
    "print(\"4. Cosine similarity captures the angle between vectors in high-dimensional space\")\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(\"Dense embeddings capture semantic similarity through:\")\n",
    "print(\"1. Shared dimensions: Both vectors have values in the same dimensions\")\n",
    "print(\"2. Similar magnitudes: Words in the same category have similar vector values\")\n",
    "print(\"3. Non-zero dot product: Unlike one-hot, dense vectors can have non-zero dot products\")\n",
    "print(\"4. Geometric proximity: Similar words are close in the embedding space\")\n",
    "print(\"5. Learned representations: Embeddings can be trained to capture semantic relationships\")\n",
    "print(\"6. Continuous values: All dimensions contribute to similarity, not just one position\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd968424",
   "metadata": {},
   "source": [
    "### 4. If vocabulary size is 100K and embedding dimension is 300, how much more efficient are dense embeddings?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2e183d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Efficiency Comparison:\n",
      "Vocabulary size: 100,000\n",
      "Embedding dimension: 300\n",
      "Sequence length: 100\n",
      "------------------------------------------------------------\n",
      "One-hot encoding: 38.15 MB\n",
      "Dense embeddings: 117.19 KB\n",
      "Efficiency ratio: 333.3x\n",
      "\n",
      "Memory per word:\n",
      "One-hot: 390.62 KB\n",
      "Dense: 1.17 KB\n",
      "Efficiency ratio: 333.3x\n",
      "\n",
      "Answer:\n",
      "For vocabulary size 100K and embedding dimension 300:\n",
      "- Dense embeddings use 333.3x less memory per sequence\n",
      "- Dense embeddings use 333.3x less memory per word\n",
      "- This means dense embeddings are approximately 333x more memory-efficient\n",
      "\n",
      "Additionally:\n",
      "- Dense embeddings capture semantic relationships (one-hot cannot)\n",
      "- Dense embeddings are less sparse (all values are used)\n",
      "- Dense embeddings enable better generalization in neural networks\n"
     ]
    }
   ],
   "source": [
    "# Calculate efficiency for 100K vocab and 300-dim embeddings\n",
    "vocab_size_large = 100000\n",
    "embedding_dim_large = 300\n",
    "seq_length_example = 100\n",
    "\n",
    "# Memory calculations\n",
    "one_hot_bytes = vocab_size_large * seq_length_example * 4\n",
    "dense_bytes = embedding_dim_large * seq_length_example * 4\n",
    "ratio = one_hot_bytes / dense_bytes\n",
    "\n",
    "print(f\"Memory Efficiency Comparison:\")\n",
    "print(f\"Vocabulary size: {vocab_size_large:,}\")\n",
    "print(f\"Embedding dimension: {embedding_dim_large}\")\n",
    "print(f\"Sequence length: {seq_length_example}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"One-hot encoding: {measure_memory(vocab_size_large, seq_length_example)}\")\n",
    "print(f\"Dense embeddings: {measure_memory(embedding_dim_large, seq_length_example)}\")\n",
    "print(f\"Efficiency ratio: {ratio:.1f}x\")\n",
    "print()\n",
    "\n",
    "# Per word comparison\n",
    "one_hot_per_word = vocab_size_large * 4\n",
    "dense_per_word = embedding_dim_large * 4\n",
    "ratio_per_word = one_hot_per_word / dense_per_word\n",
    "\n",
    "print(f\"Memory per word:\")\n",
    "print(f\"One-hot: {measure_memory(vocab_size_large, 1)}\")\n",
    "print(f\"Dense: {measure_memory(embedding_dim_large, 1)}\")\n",
    "print(f\"Efficiency ratio: {ratio_per_word:.1f}x\")\n",
    "print()\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(f\"For vocabulary size 100K and embedding dimension 300:\")\n",
    "print(f\"- Dense embeddings use {ratio:.1f}x less memory per sequence\")\n",
    "print(f\"- Dense embeddings use {ratio_per_word:.1f}x less memory per word\")\n",
    "print(f\"- This means dense embeddings are approximately {ratio:.0f}x more memory-efficient\")\n",
    "print(f\"\\nAdditionally:\")\n",
    "print(f\"- Dense embeddings capture semantic relationships (one-hot cannot)\")\n",
    "print(f\"- Dense embeddings are less sparse (all values are used)\")\n",
    "print(f\"- Dense embeddings enable better generalization in neural networks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe104b1",
   "metadata": {},
   "source": [
    "## Summary Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e417a1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Table Results:\n",
      "======================================================================\n",
      "Metric                                   One-Hot              Dense (dim=100)     \n",
      "----------------------------------------------------------------------\n",
      "Memory per word (50K vocab)              195.31 KB            400 bytes           \n",
      "Memory per sequence (100 words)          19.07 MB             39.06 KB            \n",
      "Similarity: cat-dog                      0.0000               0.9931              \n",
      "Similarity: cat-car                      0.0000               -0.5199             \n",
      "Captures semantics?                      No                   Yes                 \n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate values for summary table\n",
    "vocab_50k = 50000\n",
    "seq_100 = 100\n",
    "embed_dim = 100\n",
    "\n",
    "# Memory per word (50K vocab)\n",
    "one_hot_per_word_bytes = vocab_50k * 4\n",
    "dense_per_word_bytes = embed_dim * 4\n",
    "\n",
    "# Memory per sequence (100 words)\n",
    "one_hot_seq_bytes = vocab_50k * seq_100 * 4\n",
    "dense_seq_bytes = embed_dim * seq_100 * 4\n",
    "\n",
    "print(\"Summary Table Results:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<40} {'One-Hot':<20} {'Dense (dim=100)':<20}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Memory per word (50K vocab)':<40} {measure_memory(vocab_50k, 1):<20} {measure_memory(embed_dim, 1):<20}\")\n",
    "print(f\"{'Memory per sequence (100 words)':<40} {measure_memory(vocab_50k, seq_100):<20} {measure_memory(embed_dim, seq_100):<20}\")\n",
    "print(f\"{'Similarity: cat-dog':<40} {'0.0000':<20} {'0.9931':<20}\")\n",
    "print(f\"{'Similarity: cat-car':<40} {'0.0000':<20} {'-0.5199':<20}\")\n",
    "print(f\"{'Captures semantics?':<40} {'No':<20} {'Yes':<20}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184bde26",
   "metadata": {},
   "source": [
    "## Reflection Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce17e2e",
   "metadata": {},
   "source": [
    "### 1. What is the sparsity of one-hot vectors? (What percentage of values are zero?) How does this waste computation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62494256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Vector Sparsity:\n",
      "--------------------------------------------------\n",
      "Vocab size     100: 99.0000% zeros\n",
      "Vocab size   1,000: 99.9000% zeros\n",
      "Vocab size  10,000: 99.9900% zeros\n",
      "Vocab size  50,000: 99.9980% zeros\n",
      "Vocab size 100,000: 99.9990% zeros\n",
      "\n",
      "Answer:\n",
      "For a vocabulary of size V, one-hot vectors have (V-1)/V * 100% zeros,\n",
      "which approaches 100% for large vocabularies (e.g., 99.999% for 100K vocab).\n",
      "This wastes computation because most operations (dot products, matrix multiplications)\n",
      "involve multiplying zeros, which is computationally inefficient.\n"
     ]
    }
   ],
   "source": [
    "# Calculate sparsity for different vocab sizes\n",
    "vocab_sizes = [100, 1000, 10000, 50000, 100000]\n",
    "\n",
    "print(\"One-Hot Vector Sparsity:\")\n",
    "print(\"-\" * 50)\n",
    "for v in vocab_sizes:\n",
    "    sparsity_pct = ((v - 1) / v) * 100\n",
    "    print(f\"Vocab size {v:>7,}: {sparsity_pct:.4f}% zeros\")\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(\"For a vocabulary of size V, one-hot vectors have (V-1)/V * 100% zeros,\")\n",
    "print(\"which approaches 100% for large vocabularies (e.g., 99.999% for 100K vocab).\")\n",
    "print(\"This wastes computation because most operations (dot products, matrix multiplications)\")\n",
    "print(\"involve multiplying zeros, which is no very efficient.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73300ffa",
   "metadata": {},
   "source": [
    "### 2. Why do one-hot vectors have zero similarity? Think about what the dot product of two one-hot vectors equals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5d8abed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "Since most of the valeus in the vecotr is 0, each step in dot product mutliplication will end up with 0x0 or 0x1, which is 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAnswer:\")\n",
    "print(\"Since most of the valeus in the vecotr is 0, each step in dot product mutliplication will end up with 0x0 or 0x1, which is 0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20576cb5",
   "metadata": {},
   "source": [
    "### 3. How do dense embeddings capture \"cat is similar to dog\"? What property of the vectors enables this?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83e85593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat embedding (first 5 values): [ 0.49536443 -0.24403539  0.72994303  1.40094549 -0.21326702]\n",
      "Dog embedding (first 5 values): [ 0.57056081 -0.12112747  0.63612371  1.49291949 -0.38200557]\n",
      "Car embedding (first 5 values): [-0.55339614  0.51163881 -0.43029135 -0.36817524  0.19404485]\n",
      "\n",
      "Cat-Dog similarity: 0.9931\n",
      "Cat-Car similarity: -0.5199\n",
      "\n",
      "Answer:\n",
      "Dense embeddings capture semantic similarity because words in similar categories\n",
      "share similar vector values (high cosine similarity), enabled by the property that\n",
      "semantically related words have vectors pointing in similar directions in the embedding space.\n"
     ]
    }
   ],
   "source": [
    "cat_emb = embeddings[\"cat\"]\n",
    "dog_emb = embeddings[\"dog\"]\n",
    "cat_car_emb = embeddings[\"car\"]\n",
    "\n",
    "cat_dog_sim = cosine_similarity(cat_emb, dog_emb)\n",
    "cat_car_sim = cosine_similarity(cat_emb, cat_car_emb)\n",
    "\n",
    "print(f\"Cat embedding (first 5 values): {cat_emb[:5]}\")\n",
    "print(f\"Dog embedding (first 5 values): {dog_emb[:5]}\")\n",
    "print(f\"Car embedding (first 5 values): {cat_car_emb[:5]}\")\n",
    "print(f\"\\nCat-Dog similarity: {cat_dog_sim:.4f}\")\n",
    "print(f\"Cat-Car similarity: {cat_car_sim:.4f}\")\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(\"Words that mean simialr things or show up in similar contexts end up having vectors of relitively similar values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dc57cb",
   "metadata": {},
   "source": [
    "### 4. If vocabulary size is 100K and embedding dimension is 300, how much more efficient are dense embeddings?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ff4f790f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory per word:\n",
      "  One-hot (100K vocab): 390.62 KB\n",
      "  Dense (300 dim): 1.17 KB\n",
      "\n",
      "Efficiency ratio: 333.3x\n",
      "\n",
      "Answer:\n",
      "For vocabulary size 100K and embedding dimension 300, dense embeddings\n",
      "are 333x more memory-efficient than one-hot encoding.\n"
     ]
    }
   ],
   "source": [
    "vocab_100k = 100000\n",
    "embed_300 = 300\n",
    "\n",
    "one_hot_100k = vocab_100k * 4\n",
    "dense_300 = embed_300 * 4\n",
    "efficiency_ratio = one_hot_100k / dense_300\n",
    "\n",
    "print(f\"Memory per word:\")\n",
    "print(f\"  One-hot (100K vocab): {measure_memory(vocab_100k, 1)}\")\n",
    "print(f\"  Dense (300 dim): {measure_memory(embed_300, 1)}\")\n",
    "print(f\"\\nEfficiency ratio: {efficiency_ratio:.1f}x\")\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(f\"For vocabulary size 100K and embedding dimension 300, dense embeddings\")\n",
    "print(f\"are {efficiency_ratio:.0f}x more memory-efficient than one-hot encoding.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
