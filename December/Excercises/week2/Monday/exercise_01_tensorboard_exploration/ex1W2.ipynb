{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc0b4eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle-anthonyhay/Documents/CODE/Revature-Training/Ai-Engineering/December/venv/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 1: Setting Up TensorBoard for MNIST Classification\n",
      "======================================================================\n",
      "\n",
      "[Step 1] Loading MNIST dataset...\n",
      "Training samples: 60000\n",
      "Test samples: 10000\n",
      "\n",
      "[Step 2] Building neural network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle-anthonyhay/Documents/CODE/Revature-Training/Ai-Engineering/December/venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mnist_classifier\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"mnist_classifier\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ hidden_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hidden_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ hidden_1 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hidden_2 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Demo 01: TensorBoard Setup and Real-Time Metric Visualization\n",
    "\n",
    "This demo shows trainees how to:\n",
    "1. Install and configure TensorBoard\n",
    "2. Set up the TensorBoard callback in Keras\n",
    "3. Visualize training metrics in real-time\n",
    "4. Compare multiple experiment runs\n",
    "\n",
    "Learning Objectives:\n",
    "- Understand TensorBoard's role in model development\n",
    "- Learn to log scalars, histograms, and graphs\n",
    "- Master the TensorBoard interface for analysis\n",
    "\n",
    "References:\n",
    "- Written Content: 01-tensorboard-visualization.md\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: Basic TensorBoard Setup\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1: Setting Up TensorBoard for MNIST Classification\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load MNIST dataset\n",
    "print(\"\\n[Step 1] Loading MNIST dataset...\")\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess data\n",
    "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
    "\n",
    "print(f\"Training samples: {x_train.shape[0]}\")\n",
    "print(f\"Test samples: {x_test.shape[0]}\")\n",
    "\n",
    "# Build a simple model\n",
    "print(\"\\n[Step 2] Building neural network...\")\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,), name='hidden_1'),\n",
    "    layers.Dense(64, activation='relu', name='hidden_2'),\n",
    "    layers.Dense(10, activation='softmax', name='output')\n",
    "], name='mnist_classifier')\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db4ac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PART 2: Configuring TensorBoard Callback\n",
      "======================================================================\n",
      "\n",
      "[Step 3] TensorBoard logs will be saved to: logs/fit/20251228-123534\n",
      "\n",
      "TensorBoard Callback Configuration:\n",
      "  - Log directory: logs/fit/20251228-123534\n",
      "  - Histogram logging: Every epoch\n",
      "  - Graph visualization: Enabled\n",
      "  - Update frequency: Per epoch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# PART 2: Configure TensorBoard Callback\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 2: Configuring TensorBoard Callback\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create unique log directory with timestamp\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\n[Step 3] TensorBoard logs will be saved to: {log_dir}\")\n",
    "\n",
    "# Configure TensorBoard callback\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,        # Log weight histograms every epoch\n",
    "    write_graph=True,        # Visualize the model graph\n",
    "    write_images=False,      # Don't save model weights as images (saves space)\n",
    "    update_freq='epoch',     # Log metrics after each epoch\n",
    "    profile_batch=0          # Disable profiling (can be enabled for performance analysis)\n",
    ")\n",
    "\n",
    "print(\"\\nTensorBoard Callback Configuration:\")\n",
    "print(f\"  - Log directory: {log_dir}\")\n",
    "print(f\"  - Histogram logging: Every epoch\")\n",
    "print(f\"  - Graph visualization: Enabled\")\n",
    "print(f\"  - Update frequency: Per epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "251092ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PART 3: Training Model with TensorBoard\n",
      "======================================================================\n",
      "\n",
      "[Step 4] Starting training...\n",
      "INSTRUCTOR NOTE: Open another terminal and run:\n",
      "  tensorboard --logdir=logs/fit/20251228-123534\n",
      "  Then navigate to http://localhost:6006\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8923 - loss: 0.3827 - val_accuracy: 0.9486 - val_loss: 0.1830\n",
      "Epoch 2/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9559 - loss: 0.1521 - val_accuracy: 0.9585 - val_loss: 0.1408\n",
      "Epoch 3/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9686 - loss: 0.1060 - val_accuracy: 0.9661 - val_loss: 0.1212\n",
      "Epoch 4/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9758 - loss: 0.0812 - val_accuracy: 0.9693 - val_loss: 0.1037\n",
      "Epoch 5/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9809 - loss: 0.0630 - val_accuracy: 0.9718 - val_loss: 0.0983\n",
      "Epoch 6/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9846 - loss: 0.0509 - val_accuracy: 0.9755 - val_loss: 0.0887\n",
      "Epoch 7/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9870 - loss: 0.0418 - val_accuracy: 0.9729 - val_loss: 0.0962\n",
      "Epoch 8/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9905 - loss: 0.0329 - val_accuracy: 0.9730 - val_loss: 0.0920\n",
      "Epoch 9/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9919 - loss: 0.0276 - val_accuracy: 0.9754 - val_loss: 0.0886\n",
      "Epoch 10/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9935 - loss: 0.0227 - val_accuracy: 0.9734 - val_loss: 0.0967\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# PART 3: Train Model with TensorBoard Logging\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 3: Training Model with TensorBoard\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n[Step 4] Starting training...\")\n",
    "print(\"INSTRUCTOR NOTE: Open another terminal and run:\")\n",
    "print(f\"  tensorboard --logdir={log_dir}\")\n",
    "print(\"  Then navigate to http://localhost:6006\\n\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[tensorboard_callback],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abbe53fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PART 4: Running Multiple Experiments\n",
      "======================================================================\n",
      "\n",
      "Now let's run experiments with different configurations...\n",
      "This demonstrates TensorBoard's ability to compare runs.\n",
      "\n",
      "[Experiment 2] Training with Dropout...\n",
      "[OK] Dropout experiment complete\n",
      "\n",
      "[Experiment 3] Training larger network...\n",
      "[OK] Large network experiment complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# PART 4: Multiple Experiments for Comparison\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 4: Running Multiple Experiments\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nNow let's run experiments with different configurations...\")\n",
    "print(\"This demonstrates TensorBoard's ability to compare runs.\\n\")\n",
    "\n",
    "# Experiment 1: Baseline (already trained above)\n",
    "# Experiment 2: With Dropout\n",
    "print(\"[Experiment 2] Training with Dropout...\")\n",
    "\n",
    "log_dir_dropout = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_dropout\"\n",
    "os.makedirs(log_dir_dropout, exist_ok=True)\n",
    "\n",
    "model_dropout = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "], name='mnist_classifier_dropout')\n",
    "\n",
    "model_dropout.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "tensorboard_dropout = keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir_dropout,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True\n",
    ")\n",
    "\n",
    "history_dropout = model_dropout.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[tensorboard_dropout],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"[OK] Dropout experiment complete\")\n",
    "\n",
    "# Experiment 3: Larger Network\n",
    "print(\"\\n[Experiment 3] Training larger network...\")\n",
    "\n",
    "log_dir_large = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_large\"\n",
    "os.makedirs(log_dir_large, exist_ok=True)\n",
    "\n",
    "model_large = keras.Sequential([\n",
    "    layers.Dense(256, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "], name='mnist_classifier_large')\n",
    "\n",
    "model_large.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "tensorboard_large = keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir_large,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True\n",
    ")\n",
    "\n",
    "history_large = model_large.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[tensorboard_large],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"[OK] Large network experiment complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4f532ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PART 5: TensorBoard Analysis Guide\n",
      "======================================================================\n",
      "\n",
      "To view all experiments in TensorBoard:\n",
      "  tensorboard --logdir=logs/fit\n",
      "  Navigate to http://localhost:6006\n",
      "\n",
      "What to explore in TensorBoard:\n",
      "\n",
      "1. SCALARS Tab:\n",
      "   - Compare training/validation loss across all 3 experiments\n",
      "   - Compare accuracy curves\n",
      "   - Notice: Dropout model may have higher training loss but better validation\n",
      "   - Notice: Larger model may overfit (train/val gap)\n",
      "\n",
      "3. DISTRIBUTIONS Tab:\n",
      "   - Watch weight distributions evolve during training\n",
      "   - Notice: Healthy training shows weights spreading out\n",
      "   - Notice: Dead neurons show weights stuck near zero\n",
      "\n",
      "4. HISTOGRAMS Tab:\n",
      "   - 3D view of weight distributions over time\n",
      "   - See how each layer's weights change epoch-by-epoch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# PART 5: Analyzing Results in TensorBoard\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 5: TensorBoard Analysis Guide\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nTo view all experiments in TensorBoard:\")\n",
    "print(f\"  tensorboard --logdir=logs/fit\")\n",
    "print(f\"  Navigate to http://localhost:6006\\n\")\n",
    "\n",
    "print(\"What to explore in TensorBoard:\")\n",
    "print(\"\\n1. SCALARS Tab:\")\n",
    "print(\"   - Compare training/validation loss across all 3 experiments\")\n",
    "print(\"   - Compare accuracy curves\")\n",
    "print(\"   - Notice: Dropout model may have higher training loss but better validation\")\n",
    "print(\"   - Notice: Larger model may overfit (train/val gap)\")\n",
    "\n",
    "print(\"\\n3. DISTRIBUTIONS Tab:\")\n",
    "print(\"   - Watch weight distributions evolve during training\")\n",
    "print(\"   - Notice: Healthy training shows weights spreading out\")\n",
    "print(\"   - Notice: Dead neurons show weights stuck near zero\")\n",
    "\n",
    "print(\"\\n4. HISTOGRAMS Tab:\")\n",
    "print(\"   - 3D view of weight distributions over time\")\n",
    "print(\"   - See how each layer's weights change epoch-by-epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "550b787f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PART 6: Custom Scalar Logging\n",
      "======================================================================\n",
      "\n",
      "Demonstrating custom metric logging...\n",
      "Custom logs will be saved to: logs/custom/20251228-123802\n",
      "[OK] Custom metrics logged\n",
      "\n",
      "View custom metrics:\n",
      "  tensorboard --logdir=logs/custom/20251228-123802\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 6: Custom Scalar Logging (Advanced)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 6: Custom Scalar Logging\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nDemonstrating custom metric logging...\")\n",
    "\n",
    "log_dir_custom = \"logs/custom/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.makedirs(log_dir_custom, exist_ok=True)\n",
    "\n",
    "# Create a custom training loop to log custom metrics\n",
    "file_writer = tf.summary.create_file_writer(log_dir_custom)\n",
    "\n",
    "print(f\"Custom logs will be saved to: {log_dir_custom}\")\n",
    "\n",
    "# Simulate logging custom metrics\n",
    "with file_writer.as_default():\n",
    "    for epoch in range(10):\n",
    "        # Simulate custom metrics\n",
    "        learning_rate = 0.001 * (0.95 ** epoch)  # Decaying learning rate\n",
    "        gradient_norm = np.random.uniform(0.1, 0.5)  # Simulated gradient norm\n",
    "        \n",
    "        # Log custom scalars\n",
    "        tf.summary.scalar('learning_rate', learning_rate, step=epoch)\n",
    "        tf.summary.scalar('gradient_norm', gradient_norm, step=epoch)\n",
    "        \n",
    "        file_writer.flush()\n",
    "\n",
    "print(\"[OK] Custom metrics logged\")\n",
    "print(f\"\\nView custom metrics:\")\n",
    "print(f\"  tensorboard --logdir={log_dir_custom}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e7e8b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DEMO COMPLETE: TensorBoard Setup\n",
      "======================================================================\n",
      "\n",
      "Key Takeaways:\n",
      "1. TensorBoard visualizes training in real-time\n",
      "2. Use unique log directories for each experiment\n",
      "3. histogram_freq=1 logs weight distributions\n",
      "4. Compare multiple runs by pointing --logdir to parent folder\n",
      "5. Custom scalars enable tracking any metric\n",
      "\n",
      "======================================================================\n",
      "INSTRUCTOR NOTES\n",
      "======================================================================\n",
      "\n",
      "Show trainees:\n",
      "1. Open TensorBoard in browser while training is running\n",
      "2. Refresh to see real-time updates\n",
      "3. Use the 'Runs' selector to compare experiments\n",
      "4. Zoom and pan on loss curves\n",
      "5. Download charts as PNG/SVG for reports\n",
      "\n",
      "Common Issues:\n",
      "- Port 6006 already in use -> kill existing TensorBoard process\n",
      "- Logs not appearing -> check log_dir path is correct\n",
      "- Blank graphs -> wait for first epoch to complete\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DEMO SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DEMO COMPLETE: TensorBoard Setup\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. TensorBoard visualizes training in real-time\")\n",
    "print(\"2. Use unique log directories for each experiment\")\n",
    "print(\"3. histogram_freq=1 logs weight distributions\")\n",
    "print(\"4. Compare multiple runs by pointing --logdir to parent folder\")\n",
    "print(\"5. Custom scalars enable tracking any metric\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INSTRUCTOR NOTES\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nShow trainees:\")\n",
    "print(\"1. Open TensorBoard in browser while training is running\")\n",
    "print(\"2. Refresh to see real-time updates\")\n",
    "print(\"3. Use the 'Runs' selector to compare experiments\")\n",
    "print(\"4. Zoom and pan on loss curves\")\n",
    "print(\"5. Download charts as PNG/SVG for reports\")\n",
    "\n",
    "print(\"\\nCommon Issues:\")\n",
    "print(\"- Port 6006 already in use -> kill existing TensorBoard process\")\n",
    "print(\"- Logs not appearing -> check log_dir path is correct\")\n",
    "print(\"- Blank graphs -> wait for first epoch to complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
