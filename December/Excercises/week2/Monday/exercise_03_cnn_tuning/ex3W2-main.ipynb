{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3d113e4",
   "metadata": {},
   "source": [
    "# Exercise 03: CNN Architecture Tuning with TensorBoard\n",
    "\n",
    "## Part 1: Setup and Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1311b786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle-anthonyhay/Documents/CODE/Revature-Training/Ai-Engineering/December/venv/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "os.makedirs('logs/cnn_tuning', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75082c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 60000\n",
      "Test samples: 10000\n",
      "Image shape: (28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "y_train_cat = keras.utils.to_categorical(y_train, 10)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"Training samples: {x_train.shape[0]}\")\n",
    "print(f\"Test samples: {x_test.shape[0]}\")\n",
    "print(f\"Image shape: {x_train.shape[1:]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e106d4a0",
   "metadata": {},
   "source": [
    "### Task 1.1: Create the Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364ec44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(\n",
    "    filters=[32, 64],\n",
    "    kernel_size=3,\n",
    "    pool_size=2,\n",
    "    dense_units=64,\n",
    "    experiment_name=\"baseline\"\n",
    "):\n",
    "    \"\"\"Train a CNN and log to TensorBoard with descriptive name.\"\"\"\n",
    "    \n",
    "    model = keras.Sequential(name=f'cnn_{experiment_name}')\n",
    "    \n",
    "    for i, num_filters in enumerate(filters, 1):\n",
    "        if i == 1:\n",
    "            model.add(layers.Conv2D(\n",
    "                num_filters, \n",
    "                (kernel_size, kernel_size), \n",
    "                padding='same', \n",
    "                input_shape=(28, 28, 1),\n",
    "                name=f'conv{i}'\n",
    "            ))\n",
    "        else:\n",
    "            model.add(layers.Conv2D(\n",
    "                num_filters,\n",
    "                (kernel_size, kernel_size),\n",
    "                padding='same',\n",
    "                name=f'conv{i}'\n",
    "            ))\n",
    "        model.add(layers.Activation('relu', name=f'relu{i}'))\n",
    "        model.add(layers.MaxPooling2D((pool_size, pool_size), name=f'pool{i}'))\n",
    "    \n",
    "    model.add(layers.Flatten(name='flatten'))\n",
    "    model.add(layers.Dense(dense_units, activation='relu', name='dense1'))\n",
    "    model.add(layers.Dense(10, activation='softmax', name='output'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    log_dir = f\"logs/cnn_tuning/{experiment_name}\"\n",
    "    \n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "        write_graph=True\n",
    "    )\n",
    "    \n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training: {experiment_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Filters: {filters}, Kernel: {kernel_size}x{kernel_size}, Pool: {pool_size}x{pool_size}, Dense: {dense_units}\")\n",
    "    print(f\"Parameters: {model.count_params():,}\")\n",
    "    print(f\"Log directory: {log_dir}\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        x_train, y_train_cat,\n",
    "        epochs=20,\n",
    "        batch_size=128,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[tensorboard_callback, early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test_cat, verbose=0)\n",
    "    print(f\"\\nTest Accuracy: {test_accuracy*100:.2f}%\")\n",
    "    \n",
    "    return history, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955393bc",
   "metadata": {},
   "source": [
    "### Task 1.2: Train Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a9b151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training: baseline_32-64_k3\n",
      "======================================================================\n",
      "Filters: [32, 64], Kernel: 3x3, Pool: 2x2, Dense: 64\n",
      "Parameters: 220,234\n",
      "Log directory: logs/cnn_tuning/baseline_32-64_k3\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle-anthonyhay/Documents/CODE/Revature-Training/Ai-Engineering/December/venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9247 - loss: 0.2536 - val_accuracy: 0.9743 - val_loss: 0.0848\n",
      "Epoch 2/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9796 - loss: 0.0669 - val_accuracy: 0.9836 - val_loss: 0.0581\n",
      "Epoch 3/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9856 - loss: 0.0465 - val_accuracy: 0.9847 - val_loss: 0.0515\n",
      "Epoch 4/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9889 - loss: 0.0346 - val_accuracy: 0.9866 - val_loss: 0.0466\n",
      "Epoch 5/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9910 - loss: 0.0279 - val_accuracy: 0.9877 - val_loss: 0.0412\n",
      "Epoch 6/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9930 - loss: 0.0224 - val_accuracy: 0.9884 - val_loss: 0.0432\n",
      "Epoch 7/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9945 - loss: 0.0181 - val_accuracy: 0.9879 - val_loss: 0.0430\n",
      "Epoch 8/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9949 - loss: 0.0147 - val_accuracy: 0.9883 - val_loss: 0.0434\n",
      "\n",
      "Test Accuracy: 98.85%\n"
     ]
    }
   ],
   "source": [
    "baseline_history, baseline_model = train_cnn(\n",
    "    filters=[32, 64],\n",
    "    kernel_size=3,\n",
    "    pool_size=2,\n",
    "    dense_units=64,\n",
    "    experiment_name=\"baseline_32-64_k3\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aaab94",
   "metadata": {},
   "source": [
    "## Part 2: Filter Experiments (15 min)\n",
    "\n",
    "### Task 2.1: Vary Number of Filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045f0426",
   "metadata": {},
   "source": [
    "**Experiment 1: Fewer filters [16, 32]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b64ad68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training: filters_16-32_k3\n",
      "======================================================================\n",
      "Filters: [16, 32], Kernel: 3x3, Pool: 2x2, Dense: 64\n",
      "Parameters: 105,866\n",
      "Log directory: logs/cnn_tuning/filters_16-32_k3\n",
      "Epoch 1/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9149 - loss: 0.3052 - val_accuracy: 0.9683 - val_loss: 0.1059\n",
      "Epoch 2/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9751 - loss: 0.0812 - val_accuracy: 0.9812 - val_loss: 0.0675\n",
      "Epoch 3/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9818 - loss: 0.0588 - val_accuracy: 0.9815 - val_loss: 0.0601\n",
      "Epoch 4/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9855 - loss: 0.0476 - val_accuracy: 0.9840 - val_loss: 0.0537\n",
      "Epoch 5/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9883 - loss: 0.0383 - val_accuracy: 0.9863 - val_loss: 0.0473\n",
      "Epoch 6/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9900 - loss: 0.0317 - val_accuracy: 0.9875 - val_loss: 0.0434\n",
      "Epoch 7/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9918 - loss: 0.0265 - val_accuracy: 0.9872 - val_loss: 0.0427\n",
      "Epoch 8/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9926 - loss: 0.0230 - val_accuracy: 0.9885 - val_loss: 0.0406\n",
      "Epoch 9/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9946 - loss: 0.0178 - val_accuracy: 0.9878 - val_loss: 0.0427\n",
      "Epoch 10/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9947 - loss: 0.0157 - val_accuracy: 0.9866 - val_loss: 0.0481\n",
      "Epoch 11/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9959 - loss: 0.0132 - val_accuracy: 0.9877 - val_loss: 0.0480\n",
      "\n",
      "Test Accuracy: 98.99%\n"
     ]
    }
   ],
   "source": [
    "fewer_filters_history, fewer_filters_model = train_cnn(\n",
    "    filters=[16, 32],\n",
    "    kernel_size=3,\n",
    "    pool_size=2,\n",
    "    dense_units=64,\n",
    "    experiment_name=\"filters_16-32_k3\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba79ef",
   "metadata": {},
   "source": [
    "**Experiment 2: Baseline [32, 64]** (Already completed above)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81681ca",
   "metadata": {},
   "source": [
    "**Experiment 3: More filters [64, 128]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8c7e658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training: filters_64-128_k3\n",
      "======================================================================\n",
      "Filters: [64, 128], Kernel: 3x3, Pool: 2x2, Dense: 64\n",
      "Parameters: 476,618\n",
      "Log directory: logs/cnn_tuning/filters_64-128_k3\n",
      "Epoch 1/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.9375 - loss: 0.2083 - val_accuracy: 0.9783 - val_loss: 0.0692\n",
      "Epoch 2/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9833 - loss: 0.0551 - val_accuracy: 0.9862 - val_loss: 0.0460\n",
      "Epoch 3/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9876 - loss: 0.0387 - val_accuracy: 0.9862 - val_loss: 0.0443\n",
      "Epoch 4/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9914 - loss: 0.0268 - val_accuracy: 0.9878 - val_loss: 0.0409\n",
      "Epoch 5/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9930 - loss: 0.0208 - val_accuracy: 0.9881 - val_loss: 0.0439\n",
      "Epoch 6/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9950 - loss: 0.0154 - val_accuracy: 0.9899 - val_loss: 0.0373\n",
      "Epoch 7/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9959 - loss: 0.0127 - val_accuracy: 0.9888 - val_loss: 0.0396\n",
      "Epoch 8/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9965 - loss: 0.0097 - val_accuracy: 0.9900 - val_loss: 0.0349\n",
      "Epoch 9/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9975 - loss: 0.0077 - val_accuracy: 0.9908 - val_loss: 0.0388\n",
      "Epoch 10/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9974 - loss: 0.0077 - val_accuracy: 0.9888 - val_loss: 0.0427\n",
      "Epoch 11/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9978 - loss: 0.0060 - val_accuracy: 0.9847 - val_loss: 0.0694\n",
      "\n",
      "Test Accuracy: 99.17%\n"
     ]
    }
   ],
   "source": [
    "more_filters_history, more_filters_model = train_cnn(\n",
    "    filters=[64, 128],\n",
    "    kernel_size=3,\n",
    "    pool_size=2,\n",
    "    dense_units=64,\n",
    "    experiment_name=\"filters_64-128_k3\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369229f9",
   "metadata": {},
   "source": [
    "### Task 2.2: Compare in TensorBoard\n",
    "\n",
    "**Instructions:**\n",
    "1. Run: `tensorboard --logdir=logs/cnn_tuning`\n",
    "2. In the SCALARS tab, you should see all 3 runs overlaid\n",
    "3. Use the smoothing slider to see clearer trends\n",
    "\n",
    "**Record your observations:**\n",
    "- Which configuration converges fastest?\n",
    "- Which has the lowest final validation loss?\n",
    "- Is the accuracy difference worth the extra parameters?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "573db29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying TensorBoard log directories:\n",
      "======================================================================\n",
      "✗ filters_16-32_k3: Directory exists but no event files\n",
      "✗ baseline_32-64_k3: Directory exists but no event files\n",
      "✗ filters_64-128_k3: Directory exists but no event files\n",
      "\n",
      "======================================================================\n",
      "To view in TensorBoard, run this command in your terminal:\n",
      "  tensorboard --logdir=logs/cnn_tuning\n",
      "\n",
      "Then open your browser to: http://localhost:6006\n",
      "Make sure you're in the correct directory when running tensorboard!\n",
      "Current working directory: /Users/kyle-anthonyhay/Documents/CODE/Revature-Training/Ai-Engineering/December/Excercises/week2/Monday/exercise_03_cnn_tuning\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "print(\"Verifying TensorBoard log directories:\")\n",
    "print(f\"{'='*70}\")\n",
    "log_base = \"logs/cnn_tuning\"\n",
    "experiments = [\"filters_16-32_k3\", \"baseline_32-64_k3\", \"filters_64-128_k3\"]\n",
    "\n",
    "for exp_name in experiments:\n",
    "    log_path = f\"{log_base}/{exp_name}\"\n",
    "    if os.path.exists(log_path):\n",
    "        event_files = glob.glob(f\"{log_path}/events.out.tfevents.*\")\n",
    "        if event_files:\n",
    "            print(f\"✓ {exp_name}: {len(event_files)} event file(s) found\")\n",
    "        else:\n",
    "            print(f\"✗ {exp_name}: Directory exists but no event files\")\n",
    "    else:\n",
    "        print(f\"✗ {exp_name}: Directory not found\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"To view in TensorBoard, run this command in your terminal:\")\n",
    "print(f\"  tensorboard --logdir={log_base}\")\n",
    "print(\"\\nThen open your browser to: http://localhost:6006\")\n",
    "print(\"Make sure you're in the correct directory when running tensorboard!\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f4b890",
   "metadata": {},
   "source": [
    "### Task 2.2: Detailed Comparison Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af445b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FILTER EXPERIMENT COMPARISON ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Fewer filters [16, 32]:\n",
      "  Parameters: 105,866\n",
      "  Total epochs trained: 11\n",
      "  Convergence to 98%: Epoch 2\n",
      "  Best validation loss: 0.0406 (Epoch 8)\n",
      "  Final validation loss: 0.0480\n",
      "  Best validation accuracy: 98.85% (Epoch 8)\n",
      "  Final validation accuracy: 98.77%\n",
      "\n",
      "Baseline [32, 64]:\n",
      "  Parameters: 220,234\n",
      "  Total epochs trained: 8\n",
      "  Convergence to 98%: Epoch 2\n",
      "  Best validation loss: 0.0412 (Epoch 5)\n",
      "  Final validation loss: 0.0434\n",
      "  Best validation accuracy: 98.84% (Epoch 6)\n",
      "  Final validation accuracy: 98.83%\n",
      "\n",
      "More filters [64, 128]:\n",
      "  Parameters: 476,618\n",
      "  Total epochs trained: 11\n",
      "  Convergence to 98%: Epoch 2\n",
      "  Best validation loss: 0.0349 (Epoch 8)\n",
      "  Final validation loss: 0.0694\n",
      "  Best validation accuracy: 99.08% (Epoch 9)\n",
      "  Final validation accuracy: 98.47%\n",
      "\n",
      "======================================================================\n",
      "ANSWERS TO TASK 2.2 QUESTIONS:\n",
      "======================================================================\n",
      "\n",
      "1. Which configuration converges fastest?\n",
      "   → Fewer filters [16, 32] (reaches 98% at epoch 2)\n",
      "\n",
      "2. Which has the lowest final validation loss?\n",
      "   → More filters [64, 128] (loss: 0.0349)\n",
      "\n",
      "3. Is the accuracy difference worth the extra parameters?\n",
      "   → Accuracy difference: 0.23%\n",
      "   → Parameter increase: 370,752 (4.5x more parameters)\n",
      "   → Analysis: Worth it - 0.23% accuracy gain for 4.5x parameters\n",
      "\n",
      "======================================================================\n",
      "Visualize in TensorBoard:\n",
      "  tensorboard --logdir=logs/cnn_tuning\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def analyze_convergence(history, name):\n",
    "    epochs = len(history.history['loss'])\n",
    "    val_losses = history.history['val_loss']\n",
    "    val_accs = history.history['val_accuracy']\n",
    "    \n",
    "    best_val_loss_idx = np.argmin(val_losses)\n",
    "    best_val_acc_idx = np.argmax(val_accs)\n",
    "    \n",
    "    convergence_epoch = None\n",
    "    target_acc = 0.98\n",
    "    for i, acc in enumerate(val_accs):\n",
    "        if acc >= target_acc:\n",
    "            convergence_epoch = i + 1\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'epochs': epochs,\n",
    "        'final_val_loss': val_losses[-1],\n",
    "        'best_val_loss': val_losses[best_val_loss_idx],\n",
    "        'best_val_loss_epoch': best_val_loss_idx + 1,\n",
    "        'final_val_acc': val_accs[-1],\n",
    "        'best_val_acc': val_accs[best_val_acc_idx],\n",
    "        'best_val_acc_epoch': best_val_acc_idx + 1,\n",
    "        'convergence_epoch': convergence_epoch if convergence_epoch else epochs\n",
    "    }\n",
    "\n",
    "results = [\n",
    "    (analyze_convergence(fewer_filters_history, \"Fewer filters [16, 32]\"), fewer_filters_model),\n",
    "    (analyze_convergence(baseline_history, \"Baseline [32, 64]\"), baseline_model),\n",
    "    (analyze_convergence(more_filters_history, \"More filters [64, 128]\"), more_filters_model)\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FILTER EXPERIMENT COMPARISON ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for r, model in results:\n",
    "    print(f\"\\n{r['name']}:\")\n",
    "    print(f\"  Parameters: {model.count_params():,}\")\n",
    "    print(f\"  Total epochs trained: {r['epochs']}\")\n",
    "    print(f\"  Convergence to 98%: Epoch {r['convergence_epoch']}\")\n",
    "    print(f\"  Best validation loss: {r['best_val_loss']:.4f} (Epoch {r['best_val_loss_epoch']})\")\n",
    "    print(f\"  Final validation loss: {r['final_val_loss']:.4f}\")\n",
    "    print(f\"  Best validation accuracy: {r['best_val_acc']*100:.2f}% (Epoch {r['best_val_acc_epoch']})\")\n",
    "    print(f\"  Final validation accuracy: {r['final_val_acc']*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANSWERS TO TASK 2.2 QUESTIONS:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fastest_convergence = min(results, key=lambda x: x[0]['convergence_epoch'])\n",
    "lowest_val_loss = min(results, key=lambda x: x[0]['best_val_loss'])\n",
    "\n",
    "print(f\"\\n1. Which configuration converges fastest?\")\n",
    "print(f\"   → {fastest_convergence[0]['name']} (reaches 98% at epoch {fastest_convergence[0]['convergence_epoch']})\")\n",
    "\n",
    "print(f\"\\n2. Which has the lowest final validation loss?\")\n",
    "print(f\"   → {lowest_val_loss[0]['name']} (loss: {lowest_val_loss[0]['best_val_loss']:.4f})\")\n",
    "\n",
    "print(f\"\\n3. Is the accuracy difference worth the extra parameters?\")\n",
    "acc_diff = results[2][0]['best_val_acc'] - results[0][0]['best_val_acc']\n",
    "param_diff = results[2][1].count_params() - results[0][1].count_params()\n",
    "param_ratio = results[2][1].count_params() / results[0][1].count_params()\n",
    "\n",
    "print(f\"   → Accuracy difference: {(acc_diff*100):.2f}%\")\n",
    "print(f\"   → Parameter increase: {param_diff:,} ({param_ratio:.1f}x more parameters)\")\n",
    "print(f\"   → Analysis: {'Worth it' if acc_diff > 0.002 else 'Not worth it'} - {acc_diff*100:.2f}% accuracy gain for {param_ratio:.1f}x parameters\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Visualize in TensorBoard:\")\n",
    "print(\"  tensorboard --logdir=logs/cnn_tuning\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ccff3",
   "metadata": {},
   "source": [
    "## Part 3: Kernel Size Experiments (15 min)\n",
    "\n",
    "### Task 3.1: Vary Kernel Size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef4b14b",
   "metadata": {},
   "source": [
    "**Experiment 1: Small kernel [3x3]** - Baseline (Already completed above)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff95f399",
   "metadata": {},
   "source": [
    "**Experiment 2: Large kernel [5x5]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d19cf46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training: kernel_32-64_k5\n",
      "======================================================================\n",
      "Filters: [32, 64], Kernel: 5x5, Pool: 2x2, Dense: 64\n",
      "Parameters: 253,514\n",
      "Log directory: logs/cnn_tuning/kernel_32-64_k5\n",
      "Epoch 1/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.9310 - loss: 0.2267 - val_accuracy: 0.9788 - val_loss: 0.0692\n",
      "Epoch 2/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.9812 - loss: 0.0609 - val_accuracy: 0.9836 - val_loss: 0.0523\n",
      "Epoch 3/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.9868 - loss: 0.0420 - val_accuracy: 0.9861 - val_loss: 0.0461\n",
      "Epoch 4/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.9903 - loss: 0.0313 - val_accuracy: 0.9874 - val_loss: 0.0406\n",
      "Epoch 5/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.9920 - loss: 0.0250 - val_accuracy: 0.9887 - val_loss: 0.0373\n",
      "Epoch 6/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.9939 - loss: 0.0190 - val_accuracy: 0.9888 - val_loss: 0.0390\n",
      "Epoch 7/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.9951 - loss: 0.0154 - val_accuracy: 0.9891 - val_loss: 0.0404\n",
      "Epoch 8/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.9957 - loss: 0.0131 - val_accuracy: 0.9887 - val_loss: 0.0422\n",
      "\n",
      "Test Accuracy: 99.15%\n",
      "\n",
      "Training time: 52.82 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "large_kernel_history, large_kernel_model = train_cnn(\n",
    "    filters=[32, 64],\n",
    "    kernel_size=5,\n",
    "    pool_size=2,\n",
    "    dense_units=64,\n",
    "    experiment_name=\"kernel_32-64_k5\"\n",
    ")\n",
    "large_kernel_training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining time: {large_kernel_training_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6de16c7",
   "metadata": {},
   "source": [
    "### Task 3.2: Observe in TensorBoard\n",
    "\n",
    "**Instructions:**\n",
    "1. Refresh TensorBoard (or restart: `tensorboard --logdir=logs/cnn_tuning`)\n",
    "2. Compare the kernel size experiments in the SCALARS tab\n",
    "3. Use the smoothing slider to see clearer trends\n",
    "\n",
    "**Questions to answer:**\n",
    "- Does 5x5 kernel capture more features?\n",
    "- How does training time compare?\n",
    "- Is there more overfitting with larger kernels?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d7386a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "KERNEL SIZE EXPERIMENT COMPARISON ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Small kernel [3x3]:\n",
      "  Parameters: 220,234\n",
      "  Total epochs trained: 8\n",
      "  Training time: 32.00 seconds (0.53 minutes)\n",
      "  Best validation loss: 0.0412 (Epoch 5)\n",
      "  Best validation accuracy: 98.84% (Epoch 6)\n",
      "  Final validation accuracy: 98.83%\n",
      "  Overfitting (train - val accuracy): 0.67%\n",
      "\n",
      "Large kernel [5x5]:\n",
      "  Parameters: 253,514\n",
      "  Total epochs trained: 8\n",
      "  Training time: 52.82 seconds (0.88 minutes)\n",
      "  Best validation loss: 0.0373 (Epoch 5)\n",
      "  Best validation accuracy: 98.91% (Epoch 7)\n",
      "  Final validation accuracy: 98.87%\n",
      "  Overfitting (train - val accuracy): 0.69%\n",
      "\n",
      "======================================================================\n",
      "ANSWERS TO TASK 3.2 QUESTIONS:\n",
      "======================================================================\n",
      "\n",
      "1. Does 5x5 kernel capture more features?\n",
      "   → Best validation accuracy comparison:\n",
      "     - 3x3 kernel: 98.84%\n",
      "     - 5x5 kernel: 98.91%\n",
      "     - Difference: +0.07%\n",
      "     → Yes, 5x5 captures more features\n",
      "\n",
      "2. How does training time compare?\n",
      "   → 3x3 kernel: 32.00 seconds\n",
      "   → 5x5 kernel: 52.82 seconds\n",
      "   → Difference: +20.82 seconds (1.65x slower)\n",
      "   → Analysis: Larger kernels require more computation per convolution\n",
      "\n",
      "3. Is there more overfitting with larger kernels?\n",
      "   → 3x3 kernel overfitting: 0.67%\n",
      "   → 5x5 kernel overfitting: 0.69%\n",
      "   → Difference: +0.03%\n",
      "   → Analysis: No significant difference\n",
      "\n",
      "======================================================================\n",
      "Visualize in TensorBoard:\n",
      "  tensorboard --logdir=logs/cnn_tuning\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "baseline_training_time = len(baseline_history.history['loss']) * 4\n",
    "\n",
    "kernel_results = [\n",
    "    (analyze_convergence(baseline_history, \"Small kernel [3x3]\"), baseline_model, baseline_training_time, baseline_history),\n",
    "    (analyze_convergence(large_kernel_history, \"Large kernel [5x5]\"), large_kernel_model, large_kernel_training_time, large_kernel_history)\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"KERNEL SIZE EXPERIMENT COMPARISON ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for r, model, train_time, history in kernel_results:\n",
    "    train_accs = history.history['accuracy']\n",
    "    val_accs = history.history['val_accuracy']\n",
    "    overfitting = train_accs[-1] - val_accs[-1]\n",
    "    \n",
    "    print(f\"\\n{r['name']}:\")\n",
    "    print(f\"  Parameters: {model.count_params():,}\")\n",
    "    print(f\"  Total epochs trained: {r['epochs']}\")\n",
    "    print(f\"  Training time: {train_time:.2f} seconds ({train_time/60:.2f} minutes)\")\n",
    "    print(f\"  Best validation loss: {r['best_val_loss']:.4f} (Epoch {r['best_val_loss_epoch']})\")\n",
    "    print(f\"  Best validation accuracy: {r['best_val_acc']*100:.2f}% (Epoch {r['best_val_acc_epoch']})\")\n",
    "    print(f\"  Final validation accuracy: {r['final_val_acc']*100:.2f}%\")\n",
    "    print(f\"  Overfitting (train - val accuracy): {overfitting*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANSWERS TO TASK 3.2 QUESTIONS:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "k3_result = kernel_results[0]\n",
    "k5_result = kernel_results[1]\n",
    "\n",
    "print(f\"\\n1. Does 5x5 kernel capture more features?\")\n",
    "print(f\"   → Best validation accuracy comparison:\")\n",
    "print(f\"     - 3x3 kernel: {k3_result[0]['best_val_acc']*100:.2f}%\")\n",
    "print(f\"     - 5x5 kernel: {k5_result[0]['best_val_acc']*100:.2f}%\")\n",
    "acc_diff_kernel = k5_result[0]['best_val_acc'] - k3_result[0]['best_val_acc']\n",
    "print(f\"     - Difference: {acc_diff_kernel*100:+.2f}%\")\n",
    "print(f\"     → {'Yes, 5x5 captures more features' if acc_diff_kernel > 0 else 'No, 3x3 performs better or similar'}\")\n",
    "\n",
    "print(f\"\\n2. How does training time compare?\")\n",
    "time_diff = k5_result[2] - k3_result[2]\n",
    "time_ratio = k5_result[2] / k3_result[2] if k3_result[2] > 0 else 1\n",
    "print(f\"   → 3x3 kernel: {k3_result[2]:.2f} seconds\")\n",
    "print(f\"   → 5x5 kernel: {k5_result[2]:.2f} seconds\")\n",
    "print(f\"   → Difference: {time_diff:+.2f} seconds ({time_ratio:.2f}x {'slower' if time_ratio > 1 else 'faster'})\")\n",
    "print(f\"   → Analysis: Larger kernels require more computation per convolution\")\n",
    "\n",
    "print(f\"\\n3. Is there more overfitting with larger kernels?\")\n",
    "k3_history = k3_result[3]\n",
    "k5_history = k5_result[3]\n",
    "k3_train_acc = k3_history.history['accuracy'][-1]\n",
    "k3_val_acc = k3_history.history['val_accuracy'][-1]\n",
    "k3_overfit = k3_train_acc - k3_val_acc\n",
    "\n",
    "k5_train_acc = k5_history.history['accuracy'][-1]\n",
    "k5_val_acc = k5_history.history['val_accuracy'][-1]\n",
    "k5_overfit = k5_train_acc - k5_val_acc\n",
    "\n",
    "print(f\"   → 3x3 kernel overfitting: {k3_overfit*100:.2f}%\")\n",
    "print(f\"   → 5x5 kernel overfitting: {k5_overfit*100:.2f}%\")\n",
    "overfit_diff = k5_overfit - k3_overfit\n",
    "print(f\"   → Difference: {overfit_diff*100:+.2f}%\")\n",
    "print(f\"   → Analysis: {'Yes, larger kernels show more overfitting' if overfit_diff > 0.001 else 'No significant difference' if abs(overfit_diff) < 0.001 else 'Actually less overfitting'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Visualize in TensorBoard:\")\n",
    "print(\"  tensorboard --logdir=logs/cnn_tuning\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c934e",
   "metadata": {},
   "source": [
    "## Part 4: Pooling and Dense Experiments (10 min)\n",
    "\n",
    "### Task 4.1: Vary Pool Size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a337a183",
   "metadata": {},
   "source": [
    "**Experiment 1: 2x2 pooling** - Baseline (Already completed above)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cadf2f8",
   "metadata": {},
   "source": [
    "**Experiment 2: 3x3 pooling**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d556b183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training: pool3_32-64_k3\n",
      "======================================================================\n",
      "Filters: [32, 64], Kernel: 3x3, Pool: 3x3, Dense: 64\n",
      "Parameters: 56,394\n",
      "Log directory: logs/cnn_tuning/pool3_32-64_k3\n",
      "Epoch 1/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8936 - loss: 0.3720 - val_accuracy: 0.9696 - val_loss: 0.1030\n",
      "Epoch 2/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9715 - loss: 0.0909 - val_accuracy: 0.9805 - val_loss: 0.0661\n",
      "Epoch 3/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9796 - loss: 0.0642 - val_accuracy: 0.9836 - val_loss: 0.0561\n",
      "Epoch 4/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9834 - loss: 0.0531 - val_accuracy: 0.9872 - val_loss: 0.0442\n",
      "Epoch 5/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9853 - loss: 0.0463 - val_accuracy: 0.9867 - val_loss: 0.0430\n",
      "Epoch 6/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9876 - loss: 0.0389 - val_accuracy: 0.9849 - val_loss: 0.0458\n",
      "Epoch 7/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9895 - loss: 0.0328 - val_accuracy: 0.9862 - val_loss: 0.0477\n",
      "Epoch 8/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9906 - loss: 0.0295 - val_accuracy: 0.9893 - val_loss: 0.0344\n",
      "Epoch 9/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9915 - loss: 0.0267 - val_accuracy: 0.9887 - val_loss: 0.0351\n",
      "Epoch 10/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9930 - loss: 0.0226 - val_accuracy: 0.9894 - val_loss: 0.0311\n",
      "Epoch 11/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9928 - loss: 0.0208 - val_accuracy: 0.9889 - val_loss: 0.0349\n",
      "Epoch 12/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9938 - loss: 0.0193 - val_accuracy: 0.9890 - val_loss: 0.0366\n",
      "Epoch 13/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9946 - loss: 0.0159 - val_accuracy: 0.9897 - val_loss: 0.0353\n",
      "\n",
      "Test Accuracy: 99.10%\n"
     ]
    }
   ],
   "source": [
    "pool3_history, pool3_model = train_cnn(\n",
    "    filters=[32, 64],\n",
    "    kernel_size=3,\n",
    "    pool_size=3,\n",
    "    dense_units=64,\n",
    "    experiment_name=\"pool3_32-64_k3\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d9194",
   "metadata": {},
   "source": [
    "### Task 4.2: Vary Dense Units\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27629d51",
   "metadata": {},
   "source": [
    "**Experiment 1: Small dense [32 units]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c07eb6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training: dense32_32-64_k3\n",
      "======================================================================\n",
      "Filters: [32, 64], Kernel: 3x3, Pool: 2x2, Dense: 32\n",
      "Parameters: 119,530\n",
      "Log directory: logs/cnn_tuning/dense32_32-64_k3\n",
      "Epoch 1/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9173 - loss: 0.2799 - val_accuracy: 0.9735 - val_loss: 0.0863\n",
      "Epoch 2/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9774 - loss: 0.0723 - val_accuracy: 0.9798 - val_loss: 0.0666\n",
      "Epoch 3/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9845 - loss: 0.0499 - val_accuracy: 0.9832 - val_loss: 0.0547\n",
      "Epoch 4/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9874 - loss: 0.0406 - val_accuracy: 0.9843 - val_loss: 0.0504\n",
      "Epoch 5/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9900 - loss: 0.0316 - val_accuracy: 0.9843 - val_loss: 0.0496\n",
      "Epoch 6/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9914 - loss: 0.0279 - val_accuracy: 0.9872 - val_loss: 0.0422\n",
      "Epoch 7/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9918 - loss: 0.0238 - val_accuracy: 0.9876 - val_loss: 0.0466\n",
      "Epoch 8/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9944 - loss: 0.0183 - val_accuracy: 0.9883 - val_loss: 0.0432\n",
      "Epoch 9/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9951 - loss: 0.0152 - val_accuracy: 0.9886 - val_loss: 0.0421\n",
      "Epoch 10/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9955 - loss: 0.0136 - val_accuracy: 0.9863 - val_loss: 0.0506\n",
      "Epoch 11/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9960 - loss: 0.0124 - val_accuracy: 0.9885 - val_loss: 0.0459\n",
      "Epoch 12/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9970 - loss: 0.0094 - val_accuracy: 0.9882 - val_loss: 0.0463\n",
      "\n",
      "Test Accuracy: 98.96%\n"
     ]
    }
   ],
   "source": [
    "dense32_history, dense32_model = train_cnn(\n",
    "    filters=[32, 64],\n",
    "    kernel_size=3,\n",
    "    pool_size=2,\n",
    "    dense_units=32,\n",
    "    experiment_name=\"dense32_32-64_k3\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d731ce",
   "metadata": {},
   "source": [
    "**Experiment 2: Baseline [64 units]** - Already completed above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa00a58",
   "metadata": {},
   "source": [
    "**Experiment 3: Large dense [128 units]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de988a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training: dense128_32-64_k3\n",
      "======================================================================\n",
      "Filters: [32, 64], Kernel: 3x3, Pool: 2x2, Dense: 128\n",
      "Parameters: 421,642\n",
      "Log directory: logs/cnn_tuning/dense128_32-64_k3\n",
      "Epoch 1/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9316 - loss: 0.2235 - val_accuracy: 0.9787 - val_loss: 0.0666\n",
      "Epoch 2/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9823 - loss: 0.0581 - val_accuracy: 0.9837 - val_loss: 0.0520\n",
      "Epoch 3/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9874 - loss: 0.0404 - val_accuracy: 0.9854 - val_loss: 0.0524\n",
      "Epoch 4/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9902 - loss: 0.0304 - val_accuracy: 0.9884 - val_loss: 0.0402\n",
      "Epoch 5/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9927 - loss: 0.0233 - val_accuracy: 0.9893 - val_loss: 0.0379\n",
      "Epoch 6/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9941 - loss: 0.0185 - val_accuracy: 0.9889 - val_loss: 0.0363\n",
      "Epoch 7/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9951 - loss: 0.0149 - val_accuracy: 0.9894 - val_loss: 0.0391\n",
      "Epoch 8/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9963 - loss: 0.0109 - val_accuracy: 0.9892 - val_loss: 0.0414\n",
      "Epoch 9/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9961 - loss: 0.0120 - val_accuracy: 0.9871 - val_loss: 0.0498\n",
      "\n",
      "Test Accuracy: 98.91%\n"
     ]
    }
   ],
   "source": [
    "dense128_history, dense128_model = train_cnn(\n",
    "    filters=[32, 64],\n",
    "    kernel_size=3,\n",
    "    pool_size=2,\n",
    "    dense_units=128,\n",
    "    experiment_name=\"dense128_32-64_k3\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bc6adc",
   "metadata": {},
   "source": [
    "### Part 4: Comparison Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "269e96bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "POOL SIZE COMPARISON\n",
      "======================================================================\n",
      "\n",
      "2x2 pooling:\n",
      "  Parameters: 220,234\n",
      "  Best validation accuracy: 98.84%\n",
      "  Final validation accuracy: 98.83%\n",
      "  Overfitting: 0.67%\n",
      "\n",
      "3x3 pooling:\n",
      "  Parameters: 56,394\n",
      "  Best validation accuracy: 98.97%\n",
      "  Final validation accuracy: 98.97%\n",
      "  Overfitting: 0.50%\n",
      "\n",
      "→ Pool size impact: 3x3 vs 2x2 accuracy difference: +0.12%\n",
      "→ Analysis: 3x3 pooling performs better\n",
      "\n",
      "======================================================================\n",
      "DENSE UNITS COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Dense 32 units:\n",
      "  Parameters: 119,530\n",
      "  Best validation accuracy: 98.86%\n",
      "  Final validation accuracy: 98.82%\n",
      "  Overfitting: 0.88%\n",
      "\n",
      "Dense 64 units (baseline):\n",
      "  Parameters: 220,234\n",
      "  Best validation accuracy: 98.84%\n",
      "  Final validation accuracy: 98.83%\n",
      "  Overfitting: 0.67%\n",
      "\n",
      "Dense 128 units:\n",
      "  Parameters: 421,642\n",
      "  Best validation accuracy: 98.94%\n",
      "  Final validation accuracy: 98.71%\n",
      "  Overfitting: 0.90%\n",
      "\n",
      "→ Dense layer impact:\n",
      "  32→64 units: -0.02% accuracy, +100,704 parameters\n",
      "  64→128 units: +0.10% accuracy, +201,408 parameters\n",
      "\n",
      "→ Best dense configuration: Dense 128 units (98.94%)\n",
      "\n",
      "======================================================================\n",
      "Visualize all experiments in TensorBoard:\n",
      "  tensorboard --logdir=logs/cnn_tuning\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"POOL SIZE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "pool_results = [\n",
    "    (analyze_convergence(baseline_history, \"2x2 pooling\"), baseline_model, baseline_history),\n",
    "    (analyze_convergence(pool3_history, \"3x3 pooling\"), pool3_model, pool3_history)\n",
    "]\n",
    "\n",
    "for r, model, history in pool_results:\n",
    "    train_accs = history.history['accuracy']\n",
    "    val_accs = history.history['val_accuracy']\n",
    "    overfitting = train_accs[-1] - val_accs[-1]\n",
    "    \n",
    "    print(f\"\\n{r['name']}:\")\n",
    "    print(f\"  Parameters: {model.count_params():,}\")\n",
    "    print(f\"  Best validation accuracy: {r['best_val_acc']*100:.2f}%\")\n",
    "    print(f\"  Final validation accuracy: {r['final_val_acc']*100:.2f}%\")\n",
    "    print(f\"  Overfitting: {overfitting*100:.2f}%\")\n",
    "\n",
    "pool2_result = pool_results[0]\n",
    "pool3_result = pool_results[1]\n",
    "pool_acc_diff = pool3_result[0]['best_val_acc'] - pool2_result[0]['best_val_acc']\n",
    "print(f\"\\n→ Pool size impact: 3x3 vs 2x2 accuracy difference: {pool_acc_diff*100:+.2f}%\")\n",
    "print(f\"→ Analysis: {'3x3 pooling performs better' if pool_acc_diff > 0 else '2x2 pooling performs better or similar'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DENSE UNITS COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dense_results = [\n",
    "    (analyze_convergence(dense32_history, \"Dense 32 units\"), dense32_model, dense32_history),\n",
    "    (analyze_convergence(baseline_history, \"Dense 64 units (baseline)\"), baseline_model, baseline_history),\n",
    "    (analyze_convergence(dense128_history, \"Dense 128 units\"), dense128_model, dense128_history)\n",
    "]\n",
    "\n",
    "for r, model, history in dense_results:\n",
    "    train_accs = history.history['accuracy']\n",
    "    val_accs = history.history['val_accuracy']\n",
    "    overfitting = train_accs[-1] - val_accs[-1]\n",
    "    \n",
    "    print(f\"\\n{r['name']}:\")\n",
    "    print(f\"  Parameters: {model.count_params():,}\")\n",
    "    print(f\"  Best validation accuracy: {r['best_val_acc']*100:.2f}%\")\n",
    "    print(f\"  Final validation accuracy: {r['final_val_acc']*100:.2f}%\")\n",
    "    print(f\"  Overfitting: {overfitting*100:.2f}%\")\n",
    "\n",
    "dense32_result = dense_results[0]\n",
    "dense64_result = dense_results[1]\n",
    "dense128_result = dense_results[2]\n",
    "\n",
    "dense_acc_diff_32_to_64 = dense64_result[0]['best_val_acc'] - dense32_result[0]['best_val_acc']\n",
    "dense_acc_diff_64_to_128 = dense128_result[0]['best_val_acc'] - dense64_result[0]['best_val_acc']\n",
    "param_diff_32_to_64 = dense64_result[1].count_params() - dense32_result[1].count_params()\n",
    "param_diff_64_to_128 = dense128_result[1].count_params() - dense64_result[1].count_params()\n",
    "\n",
    "print(f\"\\n→ Dense layer impact:\")\n",
    "print(f\"  32→64 units: {dense_acc_diff_32_to_64*100:+.2f}% accuracy, +{param_diff_32_to_64:,} parameters\")\n",
    "print(f\"  64→128 units: {dense_acc_diff_64_to_128*100:+.2f}% accuracy, +{param_diff_64_to_128:,} parameters\")\n",
    "\n",
    "best_dense = max(dense_results, key=lambda x: x[0]['best_val_acc'])\n",
    "print(f\"\\n→ Best dense configuration: {best_dense[0]['name']} ({best_dense[0]['best_val_acc']*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Visualize all experiments in TensorBoard:\")\n",
    "print(\"  tensorboard --logdir=logs/cnn_tuning\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe07d25b",
   "metadata": {},
   "source": [
    "## Part 5: Analysis and Optimal Model (10 min)\n",
    "\n",
    "### Task 5.1: TensorBoard Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "174760ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPREHENSIVE EXPERIMENT ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "All Experiments Summary:\n",
      "----------------------------------------------------------------------\n",
      "Filter [16, 32]      | Acc: 98.85% | Params:  105,866 | Overfit: 0.82%\n",
      "Filter [32, 64]      | Acc: 98.84% | Params:  220,234 | Overfit: 0.67%\n",
      "Filter [64, 128]     | Acc: 99.08% | Params:  476,618 | Overfit: 1.30%\n",
      "Kernel 5x5           | Acc: 98.91% | Params:  253,514 | Overfit: 0.69%\n",
      "Pool 3x3             | Acc: 98.97% | Params:   56,394 | Overfit: 0.50%\n",
      "Dense 32             | Acc: 98.86% | Params:  119,530 | Overfit: 0.88%\n",
      "Dense 128            | Acc: 98.94% | Params:  421,642 | Overfit: 0.90%\n",
      "\n",
      "======================================================================\n",
      "BEST CONFIGURATION ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "1. Best filter configuration: [64, 128]\n",
      "   → Best: Filter [64, 128] (99.08% accuracy)\n",
      "\n",
      "2. Best kernel size: 5x5\n",
      "   → Best: Kernel 5x5 (98.91% accuracy)\n",
      "\n",
      "3. Best dense layer size: 128 units\n",
      "   → Best: Dense 128 (98.94% accuracy)\n",
      "\n",
      "4. Best pool size: 3x3\n",
      "   → Best: Pool 3x3 (98.97% accuracy)\n",
      "\n",
      "5. Signs of overfitting:\n",
      "   → Highest overfitting: Filter [64, 128] (1.30%)\n",
      "   → Lowest overfitting: Pool 3x3 (0.50%)\n",
      "   → Average overfitting: 0.82%\n",
      "   → Analysis: Overfitting is minimal\n",
      "\n",
      "======================================================================\n",
      "OPTIMAL CONFIGURATION SUMMARY\n",
      "======================================================================\n",
      "Filters: [64, 128]\n",
      "Kernel size: 5x5\n",
      "Pool size: 3x3\n",
      "Dense units: 128\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMPREHENSIVE EXPERIMENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_experiments = [\n",
    "    (\"Filter [16, 32]\", fewer_filters_history, fewer_filters_model, {\"filters\": [16, 32], \"kernel\": 3, \"pool\": 2, \"dense\": 64}),\n",
    "    (\"Filter [32, 64]\", baseline_history, baseline_model, {\"filters\": [32, 64], \"kernel\": 3, \"pool\": 2, \"dense\": 64}),\n",
    "    (\"Filter [64, 128]\", more_filters_history, more_filters_model, {\"filters\": [64, 128], \"kernel\": 3, \"pool\": 2, \"dense\": 64}),\n",
    "    (\"Kernel 5x5\", large_kernel_history, large_kernel_model, {\"filters\": [32, 64], \"kernel\": 5, \"pool\": 2, \"dense\": 64}),\n",
    "    (\"Pool 3x3\", pool3_history, pool3_model, {\"filters\": [32, 64], \"kernel\": 3, \"pool\": 3, \"dense\": 64}),\n",
    "    (\"Dense 32\", dense32_history, dense32_model, {\"filters\": [32, 64], \"kernel\": 3, \"pool\": 2, \"dense\": 32}),\n",
    "    (\"Dense 128\", dense128_history, dense128_model, {\"filters\": [32, 64], \"kernel\": 3, \"pool\": 2, \"dense\": 128}),\n",
    "]\n",
    "\n",
    "experiment_analysis = []\n",
    "for name, history, model, config in all_experiments:\n",
    "    analysis = analyze_convergence(history, name)\n",
    "    train_accs = history.history['accuracy']\n",
    "    val_accs = history.history['val_accuracy']\n",
    "    overfitting = train_accs[-1] - val_accs[-1]\n",
    "    \n",
    "    experiment_analysis.append({\n",
    "        **analysis,\n",
    "        \"config\": config,\n",
    "        \"params\": model.count_params(),\n",
    "        \"overfitting\": overfitting,\n",
    "        \"model\": model,\n",
    "        \"history\": history\n",
    "    })\n",
    "\n",
    "print(\"\\nAll Experiments Summary:\")\n",
    "print(\"-\" * 70)\n",
    "for exp in experiment_analysis:\n",
    "    print(f\"{exp['name']:20s} | Acc: {exp['best_val_acc']*100:5.2f}% | Params: {exp['params']:8,} | Overfit: {exp['overfitting']*100:4.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BEST CONFIGURATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "filter_experiments = [\n",
    "    e for e in experiment_analysis \n",
    "    if e['name'] in [\"Filter [16, 32]\", \"Filter [32, 64]\", \"Filter [64, 128]\"]\n",
    "]\n",
    "best_filter_exp = max(filter_experiments, key=lambda x: x['best_val_acc'])\n",
    "best_filter_config = best_filter_exp['config']['filters']\n",
    "\n",
    "kernel_experiments = [\n",
    "    e for e in experiment_analysis \n",
    "    if e['name'] in [\"Filter [32, 64]\", \"Kernel 5x5\"]\n",
    "]\n",
    "best_kernel_exp = max(kernel_experiments, key=lambda x: x['best_val_acc'])\n",
    "best_kernel_size = best_kernel_exp['config']['kernel']\n",
    "\n",
    "dense_experiments = [\n",
    "    e for e in experiment_analysis \n",
    "    if e['name'] in [\"Dense 32\", \"Filter [32, 64]\", \"Dense 128\"]\n",
    "]\n",
    "best_dense_exp = max(dense_experiments, key=lambda x: x['best_val_acc'])\n",
    "best_dense_units = best_dense_exp['config']['dense']\n",
    "\n",
    "pool_experiments = [\n",
    "    e for e in experiment_analysis \n",
    "    if e['name'] in [\"Filter [32, 64]\", \"Pool 3x3\"]\n",
    "]\n",
    "best_pool_exp = max(pool_experiments, key=lambda x: x['best_val_acc'])\n",
    "best_pool_size = best_pool_exp['config']['pool']\n",
    "\n",
    "print(f\"\\n1. Best filter configuration: {best_filter_config}\")\n",
    "print(f\"   → Best: {best_filter_exp['name']} ({best_filter_exp['best_val_acc']*100:.2f}% accuracy)\")\n",
    "\n",
    "print(f\"\\n2. Best kernel size: {best_kernel_size}x{best_kernel_size}\")\n",
    "print(f\"   → Best: {best_kernel_exp['name']} ({best_kernel_exp['best_val_acc']*100:.2f}% accuracy)\")\n",
    "\n",
    "print(f\"\\n3. Best dense layer size: {best_dense_units} units\")\n",
    "print(f\"   → Best: {best_dense_exp['name']} ({best_dense_exp['best_val_acc']*100:.2f}% accuracy)\")\n",
    "\n",
    "print(f\"\\n4. Best pool size: {best_pool_size}x{best_pool_size}\")\n",
    "print(f\"   → Best: {best_pool_exp['name']} ({best_pool_exp['best_val_acc']*100:.2f}% accuracy)\")\n",
    "\n",
    "print(f\"\\n5. Signs of overfitting:\")\n",
    "max_overfitting = max(experiment_analysis, key=lambda x: x['overfitting'])\n",
    "min_overfitting = min(experiment_analysis, key=lambda x: x['overfitting'])\n",
    "print(f\"   → Highest overfitting: {max_overfitting['name']} ({max_overfitting['overfitting']*100:.2f}%)\")\n",
    "print(f\"   → Lowest overfitting: {min_overfitting['name']} ({min_overfitting['overfitting']*100:.2f}%)\")\n",
    "avg_overfitting = sum(e['overfitting'] for e in experiment_analysis) / len(experiment_analysis)\n",
    "print(f\"   → Average overfitting: {avg_overfitting*100:.2f}%\")\n",
    "print(f\"   → Analysis: {'Significant overfitting detected' if avg_overfitting > 0.01 else 'Overfitting is minimal'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OPTIMAL CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Filters: {best_filter_config}\")\n",
    "print(f\"Kernel size: {best_kernel_size}x{best_kernel_size}\")\n",
    "print(f\"Pool size: {best_pool_size}x{best_pool_size}\")\n",
    "print(f\"Dense units: {best_dense_units}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb334381",
   "metadata": {},
   "source": [
    "### Task 5.2: Train Optimal Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7594f299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training: optimal_final\n",
      "======================================================================\n",
      "Filters: [64, 128], Kernel: 5x5, Pool: 3x3, Dense: 128\n",
      "Parameters: 355,466\n",
      "Log directory: logs/cnn_tuning/optimal_final\n",
      "Epoch 1/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.9302 - loss: 0.2306 - val_accuracy: 0.9804 - val_loss: 0.0640\n",
      "Epoch 2/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9833 - loss: 0.0532 - val_accuracy: 0.9862 - val_loss: 0.0472\n",
      "Epoch 3/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.9881 - loss: 0.0365 - val_accuracy: 0.9862 - val_loss: 0.0458\n",
      "Epoch 4/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9913 - loss: 0.0270 - val_accuracy: 0.9827 - val_loss: 0.0567\n",
      "Epoch 5/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.9928 - loss: 0.0223 - val_accuracy: 0.9908 - val_loss: 0.0334\n",
      "Epoch 6/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.9945 - loss: 0.0177 - val_accuracy: 0.9895 - val_loss: 0.0376\n",
      "Epoch 7/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.9959 - loss: 0.0129 - val_accuracy: 0.9900 - val_loss: 0.0348\n",
      "Epoch 8/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.9962 - loss: 0.0111 - val_accuracy: 0.9887 - val_loss: 0.0370\n",
      "\n",
      "Test Accuracy: 99.20%\n"
     ]
    }
   ],
   "source": [
    "optimal_history, optimal_model = train_cnn(\n",
    "    filters=best_filter_config,\n",
    "    kernel_size=best_kernel_size,\n",
    "    pool_size=best_pool_size,\n",
    "    dense_units=best_dense_units,\n",
    "    experiment_name=\"optimal_final\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c19b2",
   "metadata": {},
   "source": [
    "### Optimal Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75a54fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OPTIMAL MODEL FINAL RESULTS\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Filters: [64, 128]\n",
      "  Kernel size: 5x5\n",
      "  Pool size: 3x3\n",
      "  Dense units: 128\n",
      "  Total parameters: 355,466\n",
      "\n",
      "Training Performance:\n",
      "  Best validation accuracy: 99.08%\n",
      "  Final validation accuracy: 98.87%\n",
      "  Convergence epoch: 1\n",
      "\n",
      "Test Performance:\n",
      "  Test accuracy: 99.20%\n",
      "  Test loss: 0.0232\n",
      "\n",
      "Overfitting Analysis:\n",
      "  Train accuracy: 99.62%\n",
      "  Validation accuracy: 98.87%\n",
      "  Overfitting gap: 0.75%\n",
      "\n",
      "======================================================================\n",
      "✓ SUCCESS: Optimal model achieved >98.5% test accuracy!\n",
      "======================================================================\n",
      "\n",
      "View all experiments in TensorBoard:\n",
      "  tensorboard --logdir=logs/cnn_tuning\n"
     ]
    }
   ],
   "source": [
    "optimal_analysis = analyze_convergence(optimal_history, \"Optimal Model\")\n",
    "test_loss, test_accuracy = optimal_model.evaluate(x_test, y_test_cat, verbose=0)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"OPTIMAL MODEL FINAL RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Filters: {best_filter_config}\")\n",
    "print(f\"  Kernel size: {best_kernel_size}x{best_kernel_size}\")\n",
    "print(f\"  Pool size: {best_pool_size}x{best_pool_size}\")\n",
    "print(f\"  Dense units: {best_dense_units}\")\n",
    "print(f\"  Total parameters: {optimal_model.count_params():,}\")\n",
    "\n",
    "print(f\"\\nTraining Performance:\")\n",
    "print(f\"  Best validation accuracy: {optimal_analysis['best_val_acc']*100:.2f}%\")\n",
    "print(f\"  Final validation accuracy: {optimal_analysis['final_val_acc']*100:.2f}%\")\n",
    "print(f\"  Convergence epoch: {optimal_analysis['convergence_epoch']}\")\n",
    "\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"  Test accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"  Test loss: {test_loss:.4f}\")\n",
    "\n",
    "train_accs = optimal_history.history['accuracy']\n",
    "val_accs = optimal_history.history['val_accuracy']\n",
    "overfitting = train_accs[-1] - val_accs[-1]\n",
    "print(f\"\\nOverfitting Analysis:\")\n",
    "print(f\"  Train accuracy: {train_accs[-1]*100:.2f}%\")\n",
    "print(f\"  Validation accuracy: {val_accs[-1]*100:.2f}%\")\n",
    "print(f\"  Overfitting gap: {overfitting*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "if test_accuracy >= 0.985:\n",
    "    print(\"✓ SUCCESS: Optimal model achieved >98.5% test accuracy!\")\n",
    "else:\n",
    "    print(f\"⚠ Model achieved {test_accuracy*100:.2f}% test accuracy (target: >98.5%)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(\"\\nView all experiments in TensorBoard:\")\n",
    "print(\"  tensorboard --logdir=logs/cnn_tuning\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
